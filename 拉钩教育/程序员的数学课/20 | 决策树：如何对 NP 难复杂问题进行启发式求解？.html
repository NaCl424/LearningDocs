<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>20 | 决策树：如何对 NP 难复杂问题进行启发式求解？</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>20 | 决策树：如何对 NP 难复杂问题进行启发式求解？</h1><p data-nodeid="52985" class="">这一讲，我们学习决策树模型。决策树模型既可以解决分类问题，也可以解决回归问题，经典的决策树算法有 ID3、C4.5，以及 CART 算法。</p>
<p data-nodeid="52986">当今主流的人工智能模型都是基于决策树的模型，例如更复杂的梯度提升决策树、随机森林等等。这些模型有着更加复杂、深厚的数学机理，但本质上还是决策树的思想。</p>
<h3 data-nodeid="52987">决策树及其基本结构</h3>
<p data-nodeid="52988">决策树算法采用树形结构，使用层层推理来实现最终的分类。与逻辑回归不同，决策树模型很难用一个函数来描述输入向量<i><strong data-nodeid="53162">x</strong></i>和预测类别 y 之间的关系。但是，如果利用一个如下图的树形状图形，就能很轻松描述清楚。</p>
<p data-nodeid="52989"><img src="https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5HuAGijaAACMMrgsWGQ843.png" alt="图片1.png" data-nodeid="53165"></p>
<div data-nodeid="52990"><p style="text-align:center">决策树</p></div>
<p data-nodeid="52991">我们可以发现决策树有以下特点。</p>
<p data-nodeid="52992">决策树由结点和边组成。最上边的结点称作<strong data-nodeid="53176">根结点</strong>，最下边的结点称作<strong data-nodeid="53177">叶子结点</strong>。除了叶子结点外，每个结点都根据某个变量及其分界阈值，决定了是向左走或向右走。每个叶子结点代表了某个分类的结果。</p>
<ul data-nodeid="52993">
<li data-nodeid="52994">
<p data-nodeid="52995">当使用决策树模型去预测某个样本的归属类别时，需要将这个样本从根结点输入；</p>
</li>
<li data-nodeid="52996">
<p data-nodeid="52997">接着就要“按图索骥”，根据决策树中的规则，一步步找到向左走或向右走的路径；</p>
</li>
<li data-nodeid="52998">
<p data-nodeid="52999">直到最终，最终到达了某个叶子结点中，并用该叶子结点的类别表示预测结果。</p>
</li>
</ul>
<p data-nodeid="53000">例如，大迷糊的头发长度为 6 厘米、指甲长度为 0.1 厘米，我们要预测大迷糊的性别。从根结点出发，因为大迷糊的头发长度大于 5 厘米，则向左走；又因为大迷糊的指甲长度小于 1 厘米，则向右走。最终抵达叶子结点为男性，这就是预测的结果。</p>
<h3 data-nodeid="53001">决策树建模的挑战</h3>
<p data-nodeid="53002">我们曾说过，利用人工智能建模就是建立假设，再去找到假设条件下的最优化参数。对于决策树而言，它的假设就是输入向量<i><strong data-nodeid="53190">x</strong></i>和输出类别 y 之间是一棵树的条件判断关系。</p>
<p data-nodeid="53003">这样来看，决策树模型的参数就是每个结点的分裂变量和分裂变量的阈值。决策树建模，就是要找到最优的模型参数，让预测结果尽可能更准。然而，在使用决策树建模时想最优的模型参数是个 NP 难的问题。</p>
<p data-nodeid="53004">NP 难问题，指最优参数无法在多项式时间内被计算出来，这很像我们先前所说的指数爆炸。NP 难问题是数学界的一类经典问题，我们这里进行简单介绍。</p>
<p data-nodeid="53005">例如，旅行商问题（Travel Saleman Problem or TSP）就是个典型的 NP 难问题。旅行商问题，是指一个旅行商需要从 A 城市出发，经过 B 城市、C 城市、D 城市等 n 个城市后， 最后返回 A 城市，已知任意两个城市之间的路费 x<sup>ij</sup>。</p>
<p data-nodeid="53006">问：这个旅行商以怎样的城市顺序安排旅行，能让自己的路费最少。</p>
<p data-nodeid="53007">这个旅行商问题显然就是一个 NP 难问题，这体现在两个方面。</p>
<ul data-nodeid="53008">
<li data-nodeid="53009">
<p data-nodeid="53010">第一，任意给出一个行程安排，例如 A-&gt;B-&gt;D-&gt;C-&gt;A，都可以很容易算出旅行路线的总费用；</p>
</li>
<li data-nodeid="53011">
<p data-nodeid="53012">第二，但是要想找到费用最少的那条路线，最坏情况下，必须检查所有可能的路线，而这里可能的路线是 (n-1)! 个。</p>
</li>
</ul>
<blockquote data-nodeid="53013">
<p data-nodeid="53014">例如，3 个城市的路线有 A-&gt;B-&gt;C-&gt;A、A-&gt;C-&gt;B-&gt;A 两种可能，搜索空间决定了时间复杂度，显然复杂度是 O(n!)。这远大于多项式，例如 O(n)、O(n<sup>2</sup>)、O(n<sup>3</sup>) 的时间复杂度。</p>
</blockquote>
<p data-nodeid="53015">面对 NP 难问题，常规的解法是降低解的质量，去换取复杂度的降低。简而言之就是，从寻找 NP 难问题的全局最优解，转变为在多项式时间内寻找某个大差不差的次优解。通常，这类算法也被称为启发式的算法。</p>
<p data-nodeid="53016">因此，在使用决策树建模时，绝大多数的决策树算法（如 ID3 和 C4.5）所采取的策略都是启发式算法（例如贪心算法）来对空间进行搜索。这样，决策树中的每个结点都是基于当前的局部最优选择进行构造。</p>
<h3 data-nodeid="53017">ID3 决策树的启发式建模</h3>
<p data-nodeid="53018">补充完了基本概念后，我们以 ID3 决策树为例，详细探讨一下决策树建模的过程。ID3 决策树的核心思想，是在当前结点，根据信息增益最大的那个特征变量，决定如何构成决策树。</p>
<blockquote data-nodeid="53019">
<p data-nodeid="53020">我们在《10 | 信息熵：事件的不确定性如何计算？》曾经学过，利用熵、条件熵来描述事件的不确定性。进一步，可以得到信息增益，来量化某个条件对于事件不确定性降低的多少。</p>
</blockquote>
<p data-nodeid="53021">由此可见，ID3 决策树的思路非常简单，就是在所有能降低不确定性的变量中，找到那个降低程度最多的变量作为分裂变量。经过多次重复这个过程，就能得到一棵决策树了。</p>
<p data-nodeid="53022"><strong data-nodeid="53226">【ID3 决策树建模步骤】</strong></p>
<ul data-nodeid="53023">
<li data-nodeid="53024">
<p data-nodeid="53025">计算出数据集的信息熵。</p>
</li>
<li data-nodeid="53026">
<p data-nodeid="53027">对于<i><strong data-nodeid="53235">x</strong></i>向量的每一个维度：</p>
<ol data-nodeid="53028">
<li data-nodeid="53029">
<p data-nodeid="53030">以这个维度作为条件，计算条件熵；</p>
</li>
<li data-nodeid="53031">
<p data-nodeid="53032">根据数据集的信息熵和条件熵，计算信息增益。</p>
</li>
</ol>
</li>
<li data-nodeid="53033">
<p data-nodeid="53034">找到信息增益最大的变量，作为当前的分裂变量，并根据这个分裂变量得到若干个子集。</p>
</li>
<li data-nodeid="53035">
<p data-nodeid="53036">对分类过后的每个子集，递归地执行 1～3 步，直到终止条件满足。</p>
</li>
</ul>
<p data-nodeid="53037"><strong data-nodeid="53243">【ID3 决策树常见的两个终止条件】</strong></p>
<ul data-nodeid="53038">
<li data-nodeid="53039">
<p data-nodeid="53040">如果结点中的全部的样本都属于同一类别，则算法停止，并输出类别标签。</p>
</li>
<li data-nodeid="53041">
<p data-nodeid="53042">若无法继续对当层节点进行划分（特征用完），将该节点内的最高频的类别标签输出。</p>
</li>
</ul>
<h4 data-nodeid="53043">论述 ID3 建模的过程 案例 1</h4>
<p data-nodeid="53044">假设有以下数据集，每一行是一个样本，每一列一个特征变量，最后一列是样本的真实类别。试着去建立 ID3 决策树。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/9A/Ciqc1F_yin-AM7DrAABm_gQS4Ss061.png" alt="WechatIMG1513.png" data-nodeid="53251"><br>
<strong data-nodeid="53256">1.首先，计算信息熵。</strong></p>
<p data-nodeid="53045">数据集中，类别为“1”的样本有 1 个，类别为“0”的样本有 3 个；这样，类别“1”出现的概率就是 1/4，类别“0”出现的概率就是 3/4。</p>
<p data-nodeid="53046">根据公式可以知道，信息熵为<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5PqAQoX-AAAumZoxyjE562.jpg" alt="图片3.jpg" data-nodeid="53262"></p>
<p data-nodeid="53047"><strong data-nodeid="53266">2.接着，对每个变量，计算条件熵及其信息增益</strong></p>
<ul data-nodeid="53048">
<li data-nodeid="53049">
<p data-nodeid="53050">第一个变量</p>
</li>
</ul>
<p data-nodeid="53051">第一个变量，即数据集中的第一列。它包含了两个“1”和两个“0”，可见“1”和“0”的概率为1/2。其中在第一个变量为“1”的两个样本中，类别标签分别为“1”和“0”，则信息熵为<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/31/Cip5yF_q5ZWAT7kcAAAfYaOrC8U812.jpg" alt="图片4-1.jpg" data-nodeid="53272"><br>
在第一个变量为“0”的两个样本中，类别标签都是“0”，则信息熵为<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5eiAb068AAAVBDyzwWs832.jpg" alt="图片4-2.jpg" data-nodeid="53278"></p>
<p data-nodeid="53052"><img src="https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5jiAVL-sAABcTNSFB4w222.jpg" alt="图片4-3.jpg" data-nodeid="53281"></p>
<ul data-nodeid="53053">
<li data-nodeid="53054">
<p data-nodeid="53055">第二个变量</p>
</li>
</ul>
<p data-nodeid="53056">同理，可以计算出第二个变量的信息增益为<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/31/Cip5yF_q5niATTuzAAAcYoZqEvU417.jpg" alt="图片5-1.jpg" data-nodeid="53287"></p>
<ul data-nodeid="53057">
<li data-nodeid="53058">
<p data-nodeid="53059">第三个变量</p>
</li>
</ul>
<p data-nodeid="53060">对于第三个变量，它的值都是 1，也就是说第三个变量出现 1 的概率是 100%。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q5rKALD0WAABSP-kjYD0960.jpg" alt="图片5-2.jpg" data-nodeid="53293"></p>
<p data-nodeid="53061">也就是没有信息增益，等同于是个废话。从数据中也能看出，第三个变量的值对于所有数据样本而言都是一样的，可见它是没有任何区分度的。</p>
<p data-nodeid="53062"><strong data-nodeid="53298">3.变量分裂与决策树</strong></p>
<p data-nodeid="53063">基于这个过程，我们选取出信息增益最大的变量为第一个变量，标记为 x<sub>1</sub>（但若信息增益都一样，随机选择一个就可以了）。根据 x<sub>1</sub> 以及 x<sub>1</sub> 可能的取值，可以把决策树暂时建立如下图所示。</p>
<p data-nodeid="53064"><img src="https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lymqAcPlvAAA5o6I0_vQ040.png" alt="Drawing 1.png" data-nodeid="53314"></p>
<p data-nodeid="53065">根据当前的决策树，可以把原数据集切分为两个子集，分别是 D<sub>1</sub> 和 D<sub>2</sub>。</p>
<p data-nodeid="53066"><strong data-nodeid="53349">X</strong><sub>1</sub><strong data-nodeid="53350">= 0 时，子数据集是 D</strong><sub>1</sub><br>
<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q5tKAO7TMAABDmYKSNK0106.png" alt="Lark20201229-160955.png" data-nodeid="53342"><br>
在 D<sub>1</sub> 中，所有样本的类别标签都是“0”，满足了决策树建模的终止条件，则直接输出类别标签“0”，决策树更新为</p>
<p data-nodeid="53067"><img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lynCAPm2iAAAl-o0va9M449.png" alt="Drawing 2.png" data-nodeid="53353"></p>
<p data-nodeid="53068"><strong data-nodeid="53373">X</strong><sub>1</sub><strong data-nodeid="53374">= 1 时，子数据集是 D</strong><sub>2</sub><br>
<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q5uOAZn6sAAA_T_aOZE4543.png" alt="Lark20201229-161001.png" data-nodeid="53372"></p>
<p data-nodeid="53069">对于 D<sub>2</sub> 而言，还需要重复计算熵和信息增益。在D<sub>2</sub>中，类别“1”和类别“0”各有一个样本，即出现的概率都是 1/2，因此熵为<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q5yGAHP4zAAAogkrcJ6k145.jpg" alt="111.jpg" data-nodeid="53387"></p>
<ul data-nodeid="53070">
<li data-nodeid="53071">
<p data-nodeid="53072">而对于三个变量而言，第一个变量和第三个变量的信息增益都是零。这是因为，两个样本在第一个变量和第三个变量的值是相等的，没有任何信息量；</p>
</li>
<li data-nodeid="53073">
<p data-nodeid="53074">对于第二个变量而言，条件熵为 H(y|x<sub>2</sub>) = (1/2)×0 + (1/2)×0 = 0，信息增益为 g(x<sub>2</sub>,y) = H(p) - H(y|x<sub>2</sub>) = 1。</p>
</li>
</ul>
<p data-nodeid="53075">因此，应该采用第二个变量进行分裂，则有下面的决策树</p>
<p data-nodeid="53076"><img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lynmAefPmAABCjJ8vz-U348.png" alt="Drawing 3.png" data-nodeid="53422"></p>
<p data-nodeid="53077">基于这个决策树，如果 x<sub>2</sub> 为 0，则得到子集 D<sub>3</sub>；如果 x<sub>2</sub> 为 1，则得到子集 D<sub>4</sub>。</p>
<ul data-nodeid="53078">
<li data-nodeid="53079">
<p data-nodeid="53080">同时，在 D<sub>3</sub> 中，只剩下 [1,0,1,0] 这条样本，直接输出类别标签“0”；</p>
</li>
<li data-nodeid="53081">
<p data-nodeid="53082">在 D<sub>4</sub> 中，只剩下 [1,1,1,1] 这条样本，直接输出类别标签“1”。</p>
</li>
</ul>
<p data-nodeid="53083">二者都满足了停止条件，这样决策树就建立完成了，结果如下：</p>
<p data-nodeid="53084"><img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lyoCACXjYAABFsFcIBrk020.png" alt="Drawing 4.png" data-nodeid="53461"></p>
<h4 data-nodeid="53085">论述 ID3 建模的过程 案例 2</h4>
<p data-nodeid="53086">我们再看一个数据集，如下所示，这也是上一讲中，逻辑回归没有建立出模型的非线性问题的数据集。</p>
<blockquote data-nodeid="53087">
<p data-nodeid="53088">其中每一行是一个样本，每一列一个变量，最后一列是样本的类别标签。</p>
</blockquote>
<p data-nodeid="53089"><img src="https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q6E6ACKyVAABgfXtLDIA680.png" alt="图片8.png" data-nodeid="53467"><br>
我们还是可以根据 ID3 决策树的流程来建立模型。</p>
<p data-nodeid="53090"><strong data-nodeid="53473">1.首先，计算信息熵</strong></p>
<p data-nodeid="53091">我们发现在数据集中，类别为“1”的样本有两个，类别为“0”的样本也有两个；这样，他们二者出现的概率就都是 1/2。</p>
<p data-nodeid="53092"><img src="https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q6ISAWT1LAABnxl4UPqE890.png" alt="9.png" data-nodeid="53477"></p>
<p data-nodeid="53093"><strong data-nodeid="53481">2.接着，对每个变量计算条件熵和信息增益</strong></p>
<p data-nodeid="53094">对于第一个变量 x1 的值有 1 和 0 两个可能性，出现的概率都是 2/4。</p>
<p data-nodeid="53095"><img src="https://s0.lgstatic.com/i/image2/M01/04/32/Cip5yF_q6QWASKkQAAGB8GDbqqE903.png" alt="10.png" data-nodeid="53485"></p>
<p data-nodeid="53096"><strong data-nodeid="53489">3.变量分裂与决策树</strong></p>
<p data-nodeid="53097">当信息增益完全一致的时候，我们随机选择一个作为分裂变量。假设选 x<sub>1</sub>，则根据 x<sub>1</sub> 的不同，可以得到下面的决策树。</p>
<p data-nodeid="53098"><img src="https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lyouAEIX-AAAjk2gZiFs803.png" alt="Drawing 5.png" data-nodeid="53501"></p>
<p data-nodeid="53099">根据当前的决策树，可以将数据集分割为 D1 和 D2 两部分，并建立决策树。</p>
<p data-nodeid="53100"><strong data-nodeid="53506">X1 为 0 时，子数据集为 D1</strong></p>
<p data-nodeid="53101"><img src="https://s0.lgstatic.com/i/image/M00/8C/59/CgqCHl_q6USARgYZAABBkd-FmD0667.png" alt="Lark20201229-161007.png" data-nodeid="53509"></p>
<p data-nodeid="53102"><img src="https://s0.lgstatic.com/i/image/M00/8C/4E/Ciqc1F_q6VaAIHAuAAA4MCT2o0g658.png" alt="Lark20201229-161010.png" data-nodeid="53512"></p>
<ul data-nodeid="53103">
<li data-nodeid="53104">
<p data-nodeid="53105">不难发现，在 D<sub>1</sub> 中，第一个变量 x<sub>1</sub> 和第三个变量 x<sub>3</sub> 的信息增益都是 0；</p>
</li>
<li data-nodeid="53106"></li>
</ul>
<p data-nodeid="53107"><img src="https://s0.lgstatic.com/i/image/M00/8C/4F/Ciqc1F_q7EWARz2DAAA7OEkAWV8268.png" alt="Lark20201229-164322.png" data-nodeid="53528"></p>
<p data-nodeid="53108">可见，需要用 x<sub>2</sub> 对 D<sub>1</sub> 进行拆分，这样就得到了下面的决策树。</p>
<p data-nodeid="53109"><img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lypOAA_3YAABDonV9NLU584.png" alt="Drawing 6.png" data-nodeid="53540"></p>
<p data-nodeid="53110"><strong data-nodeid="53560">x</strong><sub>1</sub><strong data-nodeid="53561">为 1 时，子数据集是 D</strong><sub>2</sub><br>
<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/33/CgpVE1_q6ayAOUvvAABBfCLOlU8611.png" alt="Lark20201229-161012.png" data-nodeid="53559"></p>
<p data-nodeid="53111">对于 D<sub>2</sub> 子集，也用同样的方法，我们直接给出建树的结果如下：</p>
<p data-nodeid="53112"><img src="https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lypqAGGukAABa0m8jZLc584.png" alt="Drawing 7.png" data-nodeid="53569"></p>
<p data-nodeid="53113">所剩特征为 0，分裂结束。</p>
<h4 data-nodeid="53114">ID3 决策树的代码实现</h4>
<p data-nodeid="53115">对于这种像 ID3 这种成型的算法而言，已经有很多被封装好的工具包（如 sklearn）可以直接调用，并不需要自己来自主开发。</p>
<p data-nodeid="53116">如果自己来写底层建模的代码，可能需要上百行的代码量。为了给大家展示最核心的部分，我们给出建立 ID3 决策树的伪代码。</p>
<pre class="lang-python" data-nodeid="53117"><code data-language="python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createTree</span>(<span class="hljs-params">x, y</span>):</span>
	<span class="hljs-keyword">if</span> 终止条件满足:
		<span class="hljs-keyword">return</span> labels[<span class="hljs-number">0</span>]
	hp = getHp(y)
	xStar = getBestSplitVar(x,y)
	model.save(xStar)
	xSubList = getSubset(xStar,x)
	ySubList = getSubset(xStar,y)
	<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> len(xSubList):
		createTree(xSubList[i],ySubList[i])
	<span class="hljs-keyword">return</span> model
</code></pre>
<p data-nodeid="53118">【我们对代码进行走读】</p>
<p data-nodeid="53119">从开发的角度来看，决策树采用了一种递归式的建模，可见函数主体一定是一个递归结构。这个递归的终止条件，就是 ID3 建树的终止条件。</p>
<ul data-nodeid="53120">
<li data-nodeid="53121">
<p data-nodeid="53122">第 3 行，我们在伪代码中，只提及了所有样本一致的情况篇；另一种情况比较少见，可以先不处理。</p>
</li>
<li data-nodeid="53123">
<p data-nodeid="53124">第 4 行，我们需要开发个函数 getHp() 来计算当前数据集的熵，计算熵只跟类别标签 y 向量有关。</p>
</li>
<li data-nodeid="53125">
<p data-nodeid="53126">第 5 行，我们需要对所有的变量计算条件熵，并比较出谁产生的信息增益最大。此时我们需要开发 getBestSplitVar() 的函数，它同时依赖 x 向量和 y 向量的输入。</p>
</li>
<li data-nodeid="53127">
<p data-nodeid="53128">在得到了最优的分裂变量后，我们就完成了一次迭代，可以在第 6 行把它保存在模型中了。</p>
</li>
<li data-nodeid="53129">
<p data-nodeid="53130">第 7 行和第 8 行，是基于现有模型，对数据集进行的切分。此时还需要开发一个函数 getSubset()，需要实现的功能是在数据集中基于 xStar 对数据集进行分割，并返回所有子集的 list。</p>
</li>
<li data-nodeid="53131">
<p data-nodeid="53132">最后，第 9～10 行，对于每个子集，递归地调用建树的函数 createTree()，再次重复上面的过程。</p>
</li>
</ul>
<p data-nodeid="53133">ID3 决策树建树的代码开发，就是一个递归结构的开发。虽然实际的开发中需要开发多个函数，代码量也是很多的，但从原理来看还是非常简单的。</p>
<h3 data-nodeid="53134">决策树模型的优势和不足</h3>
<h4 data-nodeid="53135">1.优势</h4>
<p data-nodeid="53136">从上述结果可以看出，决策树最大的优势，是在原本逻辑回归无法做出准确分类的数据集上，决策树<strong data-nodeid="53590">可以做出正确分类</strong>。</p>
<ul data-nodeid="53137">
<li data-nodeid="53138">
<p data-nodeid="53139">这是因为，逻辑回归方法得到的决策边界总是线性的，它是个只能处理线性问题的线性模型；</p>
</li>
<li data-nodeid="53140">
<p data-nodeid="53141">而决策树是按照层次结构的规则生成的，它可以通过增加决策树的层次来模拟更复杂的分类边界，可以用来解决更复杂的非线性问题。</p>
</li>
</ul>
<p data-nodeid="53142">同时，在模型的可解释性上，决策树明确给出了预测的依据。要解释决策树如何预测非常简单，从根结点开始，依照所有的特征开始分支，一直到到达叶子节点，找到最终的预测。决策树可以很好地捕捉特征之间的互动和依赖，树形结构也可以很好地可视化。</p>
<h4 data-nodeid="53143">2.不足</h4>
<p data-nodeid="53144">ID3 决策树，或者说绝大多数的决策树都不是最优的树结构。这主要是因为建树本来就是个 NP 难问题，导致我们的算法只能采用一些启发式的贪心算法。从一开始，建树的目标就不是去寻找最优解。</p>
<h3 data-nodeid="53145">小结</h3>
<p data-nodeid="53146">决策树模型是<strong data-nodeid="53602">浅层模型</strong>中最优秀、最普适的一类模型。很多提升方法也都是基于决策树演变而来的。</p>
<p data-nodeid="53147">在这里我们提到了一个浅层模型的概念，这主要是与深度学习进行的比较。我们知道这几年由于神经网络的兴起，深度学习的概念一下子称为 AI 领域的研究热点。</p>
<p data-nodeid="53148">原本，学者们并没有浅层模型的概念。因为深度学习兴起后，产生了很多层次复杂、结构很深的模型；那么与之对应的经典模型，就被人们统称为浅层模型了。</p>
<p data-nodeid="53149">然而经过人们的验证会发现，浅层模型中的佼佼者仍然是<strong data-nodeid="53612">树模型</strong>。而深层模型通过增加了模型的复杂度，换取了更好的效果。关于深层模型，我们会在下一讲《21 | 神经网络与深度学习：计算机是如何理解图像、文本和语音的？》中进行讨论。</p>
<p data-nodeid="53150">最后，我们留一个练习题。对于下面的数据集，试着用 ID3 算法建立决策树。</p>
<p data-nodeid="54885"><img src="https://s0.lgstatic.com/i/image2/M01/04/32/Cip5yF_q6gyAUHnuAABhPZjS8Mk947.png" alt="图片13.png" data-nodeid="54890"></p>
<hr data-nodeid="54886">
<p data-nodeid="54887" class="te-preview-highlight"><a href="https://wj.qq.com/s2/7812549/4cd8/" data-nodeid="54893">课程评价入口，挑选 5 名小伙伴赠送小礼品～</a></p></div>

</body></html>