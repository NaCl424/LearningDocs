<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>19 | 逻辑回归：如何让计算机做出二值化决策？</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>19 | 逻辑回归：如何让计算机做出二值化决策？</h1><p data-nodeid="48070" class="">在上一讲，学习完 AI 的基本框架后，我们现在就开始围绕当前人工智能领域最常用的模型，来分别学习一下它们背后的原理。</p>
<p data-nodeid="48071">这一讲，我们从最常见的逻辑回归模型说起，逻辑回归是人工智能领域中入门级的基础模型，它在很多领域都有应用，例如用户的信贷模型、疾病识别等。</p>
<p data-nodeid="48072">逻辑回归是一种分类模型，可以对一个输入 x，识别并预测出一个二值化的类别标签 y。例如，要预测照片中人物的性别，可以采用逻辑回归建立模型。给模型输入一个描述照片的特征向量 x，经过模型的计算，可以得到输出值 y 为“男”或“女”。</p>
<p data-nodeid="48073">在深入学习逻辑回归的原理之前，我们先来了解一下什么是分类问题，以及分类问题有哪些类型。</p>
<h3 data-nodeid="48074">分类问题</h3>
<p data-nodeid="48075">在人工智能领域中，分类问题是特别常见的一种问题类型。简而言之，分类问题就是对一个测试验本去预测它归属的类别。例如，预测胎儿性别、预测足球比赛结果。</p>
<p data-nodeid="48076">根据归属类别可能性的数量，分类问题又可以分为二分类问题和多分类问题。</p>
<ul data-nodeid="48077">
<li data-nodeid="48078">
<p data-nodeid="48079">二分类问题，顾名思义就是预测的归属类别只有两个。例如，预测性别男/女、预测主场球队的胜负、预测明天是否下雨。</p>
</li>
<li data-nodeid="48080">
<p data-nodeid="48081">多分类问题，预测的归属类别大于两个的那类问题。例如，预测足球比赛结果是胜、负，还是平局；预测明天天气是雨天、晴天，还是阴天。</p>
</li>
</ul>
<p data-nodeid="48082">在研究分类的建模算法时，人们往往会从二分类问题入手，这主要是因为多分类问题可以用多个二分类问题来表示。例如，预测明天天气是雨天、晴天，还是阴天，这是个多分类问题（三分类）；它也可以表示为，预测明天是否下雨、预测明天是否晴天、预测明天是否阴天，这三个二分类问题。</p>
<p data-nodeid="48083">因此，二分类问题是分类问题的基础，在讨论分类算法时，人们往往会从二分类问题入手。</p>
<h3 data-nodeid="48084">逻辑回归及其建模流程</h3>
<p data-nodeid="48085">逻辑回归（Logistic Regression，LR）是人工智能领域非常经典的算法之一，它可以用来对二分类问题进行建模，对于一个给定的输入，可以预测其类别为正 1 或负 0。接下来，我们就从 AI 基本框架的 3 个公式，来学习一下 LR 的建模流程。</p>
<p data-nodeid="48086">重温一下人工智能基本框架的 3 个公式分别是：</p>
<ul data-nodeid="48087">
<li data-nodeid="48088">
<p data-nodeid="48089">第一步，根据假设，写出模型的输入、输出关系 y = f(<i><strong data-nodeid="48222">w</strong></i>; x)；</p>
</li>
<li data-nodeid="48090">
<p data-nodeid="48091">第二步，根据偏差的计算方法，写出描述偏差的损失函数 L(<i><strong data-nodeid="48230">w</strong></i>)；</p>
</li>
<li data-nodeid="48092">
<p data-nodeid="48093">第三步，对于损失函数，求解最优的参数值，即 <i><strong data-nodeid="48245">w</strong></i>*= argmin L(<i><strong data-nodeid="48246">w</strong></i>)。</p>
</li>
</ul>
<p data-nodeid="48094">接下来，我会逐一展示这三步的过程。</p>
<h4 data-nodeid="48095">1.模型的输入、输出关系（Sigmoid 函数）</h4>
<p data-nodeid="48096">在逻辑回归中，第一个公式的表达式非常简单，为 y=f(<i><strong data-nodeid="48271">w;x</strong></i>)=sigmoid(<i><strong data-nodeid="48272">w·x</strong></i>)=1/(1+e<sup>-<i><strong data-nodeid="48273">w·x</strong></i></sup>)。</p>
<p data-nodeid="48097">直观上来看，逻辑回归的模型假设是，把模型参数向量 <i><strong data-nodeid="48287">w</strong></i> 和输入向量 <i><strong data-nodeid="48288">x</strong></i> 的点乘（即线性变换）结果输入给 Sigmoid 函数中，即可得到预测值 y。</p>
<p data-nodeid="48098">此时的预测值 y 还是个 0～1 之间的连续值，这是因为 Sigmoid 函数的值域是 (0,1)。逻辑回归是个二分类模型，它的最终输出值只能是两个类别标签之一。通常，我们习惯于用“0”和“1”来分别标记二分类的两个类别。</p>
<p data-nodeid="48099">在逻辑回归中，常用预测值 y 和 0.5 的大小关系，来判断样本的类别归属。<strong data-nodeid="48294">具体地，预测值 y 如果大于 0.5，则认为预测的类别为 1；反之，则预测的类别为 0。</strong></p>
<p data-nodeid="48100">我们把上面的描述进行总结，来汇总一下逻辑回归输入向量、预测值和类别标签之间的关系，则有下面的流程图。</p>
<p data-nodeid="48101"><img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxJuAJjk2AAE7vKJA3Cg656.png" alt="图片1.png" data-nodeid="48298"></p>
<p data-nodeid="48102">为了深入了解逻辑回归的模型假设，我们需要先认识下 Sigmoid 函数。Sigmoid 函数的表达式为 y = sigmoid(x)=1/(1+e<sup>-x</sup>)，它是个单调递增函数，定义域为 (-∞, +∞)，值域为 (0,1)，它的函数图像如下。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxKuAaTt2AAEuW5MrG8c228.png" alt="图片2.png" data-nodeid="48307"><br>
我们可以看出，Sigmoid 函数可以将任意一个实数 x，单调地映射到 0 到 1 的区间内，这正好符合了“概率”的取值范围。</p>
<p data-nodeid="48103">我们还可以用求导公式来看一下 Sigmoid 函数的一阶导数。<br>
<img src="https://s0.lgstatic.com/i/image2/M01/04/15/Cip5yF_pVFqAQwAPAAA8cpQoHKA089.png" alt="WechatIMG1422.png" data-nodeid="48314"></p>
<h4 data-nodeid="48104">2.逻辑回归的损失函数</h4>
<p data-nodeid="48105">有了这些基本假设后，我们尝试根据偏差的计算方法，写出描述偏差的损失函数 L(<i><strong data-nodeid="48323">w</strong></i>)。</p>
<p data-nodeid="48106">我们刚刚提到过，逻辑回归预测结果的值域 y 为 (0,1)，代表的是样本属于类别 1 的概率。</p>
<ul data-nodeid="48107">
<li data-nodeid="48108">
<p data-nodeid="48109">具体而言，如果样本属于类别“1”的概率大于 0.5，则认为样本的预测类别为“1”；</p>
</li>
<li data-nodeid="48110">
<p data-nodeid="48111">如果样本属于类别“1”的概率小于 0.5，则认为样本的预测类别为“0”。</p>
</li>
</ul>
<p data-nodeid="48112">这里出现了这么多的概率，我们可以借鉴在《09 | 似然估计：如何利用 MLE 对参数进行估计？》中学的概率计算和极大似然估计的思想，尝试写出样本被正确预测的概率。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/3D/CgqCHl_pVHaAOfpZAAC78iA7-nA342.png" alt="WechatIMG1421.png" data-nodeid="48333"><br>
我们将上面两个等式合并，就可以得到某个数据<i><strong data-nodeid="48379">x</strong></i><sub>i</sub> 被正确预测的概率，即 P(y<sub>i</sub>|x<sub>i</sub>,w)=Φ(z<sub>i</sub>)<sup>y<sub>i</sub></sup>·[1-Φ(z<sub>i</sub>)]<sup>1-y<sub>i</sub></sup>。</p>
<ul data-nodeid="48113">
<li data-nodeid="48114">
<p data-nodeid="48115">如果真实结果 y<sub>i</sub> 为 1，则 P(y<sub>i</sub>|x<sub>i</sub>,w) = Φ(z<sub>i</sub>)，描述的是样本被预测为类别“1”的概率；</p>
</li>
<li data-nodeid="48116">
<p data-nodeid="48117">如果真实结果 y<sub>i</sub> 为 0，则 P(y<sub>i</sub>|x<sub>i</sub>,w) = 1-Φ(z<sub>i</sub>)，描述的是样本被预测为类别“0”的概率。</p>
</li>
</ul>
<p data-nodeid="48118">接下来可以将上式扩展到整个样本数据集中，则可采用极大似然估计得到 L(<i><strong data-nodeid="48427">w</strong></i>)，即<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lxSyACUiYAAA4FTEM9qs111.png" alt="图片4.png" data-nodeid="48426"></p>
<p data-nodeid="48119">我们之前在《09 | 似然估计：如何利用 MLE 对参数进行估计？》学习极大似然估计 MLE 时，曾经提过一个常用的公式化简方法，那就是通过取对数，让连续乘积的大型运算变为连续求和，则有<br>
<img src="https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lxTyASfq6AAA1M4H_6RI019.png" alt="图片5.png" data-nodeid="48434"></p>
<h4 data-nodeid="48120"><strong data-nodeid="48438">3.求解最优的模型参数值</strong></h4>
<p data-nodeid="48121">AI 建模框架的最后一步，就是对损失函数求解最优的参数值，即<i><strong data-nodeid="48497">w</strong></i>*= argmin l(<i><strong data-nodeid="48498">w</strong></i>)。刚刚我们求得，损失函数为<br>
<img src="https://s0.lgstatic.com/i/image2/M01/03/FE/CgpVE1_lxUmAc_dEAAA5bSwYYTY904.png" alt="图片6.png" data-nodeid="48456"><br>
可见，损失函数是个关于 <i><strong data-nodeid="48499">x</strong></i><sub>i</sub>、y<sub>i</sub> 和 <i><strong data-nodeid="48500">w</strong></i> 的函数，而<i><strong data-nodeid="48501">x</strong></i><sub>i</sub> 和 y<sub>i</sub> 是输入数据集中已知的条件，所以损失函数的未知数只有 <i><strong data-nodeid="48502">w</strong></i>。</p>
<p data-nodeid="48122">于是可以得到结论，逻辑回归最后一步的建模公式，实质上就是求解函数极值的问题。</p>
<blockquote data-nodeid="48123">
<p data-nodeid="48124">关于求极值，我们在《05 | 求极值：如何找到复杂业务的最优解？》曾详细介绍过求导法和梯度下降法。</p>
</blockquote>
<p data-nodeid="48125">在这里，由于损失函数包含了非线性的 sigmoid 函数，求导法是无法得到解析解的；因此，我们使用梯度下降法来求解参数<i><strong data-nodeid="48524">w</strong></i>。<br>
<img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxYeAZOMOAADJ2CJBEHc898.png" alt="图片7.png" data-nodeid="48517"><br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxb2ALAYhAACvGE2H5Rw210.png" alt="图片8.png" data-nodeid="48521"><br>
我们已经计算出了损失函数关于模型参数的导数，这也是损失函数的梯度方向，我们可以利用先前所学的梯度下降法来求解函数的极值。</p>
<p data-nodeid="48126">然而，这里存在一个计算效率的缺陷，即梯度函数中包含了大型求和的运算。这里的大型求和是 i 从 1 到 n 的计算，也就是对于整个数据集全部的数据去进行的全量计算。</p>
<blockquote data-nodeid="48127">
<p data-nodeid="48128">可以想象出，当输入的数据量非常大的时候，梯度下降法每次的迭代都会产生大量的计算。这样，建模过程中会消耗大量计算资源，模型更新效率也会受到很大影响。</p>
</blockquote>
<h4 data-nodeid="48129">【随机梯度下降法】</h4>
<p data-nodeid="48130">为了解决这个问题，人工智能领域常常用<strong data-nodeid="48541">随机梯度下降法</strong>来修正<strong data-nodeid="48542">梯度下降法</strong>的不足。随机梯度下降法与梯度下降法的区别只有一点，那就是随机梯度下降在每轮更新参数时，只随机选取一个样本 d<sub>m</sub> 来计算梯度，而非计算整个数据集梯度。其余的计算过程，二者完全一致。</p>
<p data-nodeid="48131"><img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxdaAQ749AADJ2O9gnkU384.png" alt="图片9.png" data-nodeid="48545"></p>
<p data-nodeid="48132">根据上面更新公式的算法，我们通过多轮迭代，就能最终求解出让 l(w) 取得最大值的参数向量<i><strong data-nodeid="48553">w</strong></i>。</p>
<h3 data-nodeid="48133">逻辑回归代码实现</h3>
<p data-nodeid="48134">接下来，我们在下面的数据集上，分别采用逻辑回归来建立分类模型。</p>
<p data-nodeid="48135">第一个数据集如下，其中每一行是一个样本，每一列一个特征，最后一列是样本的类别。</p>
<p data-nodeid="48136"><img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxgCAcT3PAADKvRlRWh8558.png" alt="图片10.png" data-nodeid="48559"></p>
<p data-nodeid="48137">第二个数据集如下，格式与第一个数据集相同。</p>
<p data-nodeid="48138"><img src="https://s0.lgstatic.com/i/image2/M01/03/FC/Cip5yF_lxgaAbVaDAADgP88xevs672.png" alt="图片11.png" data-nodeid="48563"></p>
<p data-nodeid="48139">我们采用下面的代码，建立逻辑回归模型。</p>
<pre class="lang-python" data-nodeid="48140"><code data-language="python"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> random
x = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]])
y = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])
<span class="hljs-comment">#y = np.array([0,0,1,1])</span>
w = np.array([<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>,<span class="hljs-number">0.5</span>])
a = <span class="hljs-number">0.01</span>
maxloop = <span class="hljs-number">10000</span>
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(maxloop):
	m = random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">3</span>)
	fi = <span class="hljs-number">1.0</span>/(<span class="hljs-number">1</span>+math.pow(math.e,-np.dot(x[m],w)))
	g = (y[m] - fi)*x[m]
	w = w + a*g
	<span class="hljs-keyword">print</span> w
</code></pre>
<p data-nodeid="48141">【我们对代码进行走读】</p>
<ul data-nodeid="48142">
<li data-nodeid="48143">
<p data-nodeid="48144">代码中，第 5～7 行分别输入数据集 x 和 y；</p>
</li>
<li data-nodeid="48145">
<p data-nodeid="48146">第 9 行，初始化参数向量，在这里，我们采用固定的初始化方法，你也可以调整为随机初始化；</p>
</li>
<li data-nodeid="48147">
<p data-nodeid="48148">第 11 行，设置学习率为 0.01；</p>
</li>
<li data-nodeid="48149">
<p data-nodeid="48150">第 12 行，设置最大迭代轮数为 10000 次。</p>
</li>
</ul>
<p data-nodeid="48151">接下来进入随机梯度下降法的循环体。</p>
<ul data-nodeid="48152">
<li data-nodeid="48153">
<p data-nodeid="48154">第 14 行，从 0 到 3 中随机抽取一个数字作为本轮迭代梯度的样本；</p>
</li>
<li data-nodeid="48155">
<p data-nodeid="48156">第 15 行，计算 Φ(z<sub>m</sub>)；</p>
</li>
<li data-nodeid="48157">
<p data-nodeid="48158">第 16 行，计算样本 m 带来的梯度 g；</p>
</li>
<li data-nodeid="48159">
<p data-nodeid="48160">第 17 行，利用随机梯度下降法更新参数 w；</p>
</li>
<li data-nodeid="48161">
<p data-nodeid="48162">第 18 行，打印这一轮的结果。</p>
</li>
</ul>
<h4 data-nodeid="48163">【数据集一】</h4>
<p data-nodeid="48164">运行上述代码，我们对数据集一建模得到的最优参数为 [3.1,3.0,-4.8]。利用这组参数，我们可以对数据集一的学习效果进行测试，如下表所示</p>
<p data-nodeid="48165"><img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lxhOAFnR4AAIK9izy6Cs182.png" alt="图片12.png" data-nodeid="48588"></p>
<p data-nodeid="48166">可见，数据集一上，我们的模型全部正确预测，效果非常好。</p>
<h4 data-nodeid="48167">【数据集二】</h4>
<p data-nodeid="48168">再运行上述代码，我们对数据集二建模得到的最优参数为 [0.16, 0.10, -0.03]。利用这组参数，我们可以对数据集二的学习效果进行测试，如下表所示。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/19/Ciqc1F_lxh2AB8_FAAIMnwsJ144124.png" alt="图片13.png" data-nodeid="48599"><br>
我们发现，在数据集二上，模型的预测结果只能是马马虎虎，这体现在两点：</p>
<ul data-nodeid="48169">
<li data-nodeid="48170">
<p data-nodeid="48171">4 个样本中，并没有全部正确预测，样本 1 预测错误；</p>
</li>
<li data-nodeid="48172">
<p data-nodeid="48173">对于正确预测的 3 个样本而言，预测值都在边界线 0.5 附近，就算是正确预测，也没有压倒性优势。</p>
</li>
</ul>
<p data-nodeid="48174">那么为什么同样的模型，只是换了数据集，效果就千差万别呢？</p>
<h3 data-nodeid="48175">逻辑回归的不足</h3>
<p data-nodeid="48176">这是因为，逻辑回归是个线性模型，它只能处理线性问题。</p>
<p data-nodeid="48177">例如，对于一个二维平面来说，线性模型就是一条直线。如果数据的分布不支持用一条线来分割的话，逻辑回归就无法收敛，如下图所示。<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/24/CgqCHl_lximANu7DAAC6Cmxv9rM944.png" alt="图片3.png" data-nodeid="48611"><br>
图中蓝色点是一类，黄色点是一类。现在，我们要用逻辑回归这样的线性模型来进行区分。可见，不论这条线怎么选，都是无法将两类样本进行分割的，这也是逻辑回归模型的缺陷。</p>
<blockquote data-nodeid="48178">
<p data-nodeid="48179">要想解决的话，只有用更复杂的模型，例如我们后续的课时中会介绍的决策树、神经网络等模型。</p>
</blockquote>
<h4 data-nodeid="48180">【逻辑回归与线性回归】</h4>
<p data-nodeid="48181">在上一讲《18 | AI 入门：利用 3 个公式搭建最简 AI 框架》中，通过“身高预测”，我们从人工智能模型的视角，重新认识了线性回归，那么逻辑回归和线性回归的不同有哪些呢？</p>
<ul data-nodeid="48182">
<li data-nodeid="48183">
<p data-nodeid="48184"><strong data-nodeid="48622">从名字上比较</strong></p>
</li>
</ul>
<p data-nodeid="48185">线性回归是回归模型，是用一根“线”去回归出输入和输出之间的关系，即用一根线去尽可能把全部样本“串”起来。</p>
<p data-nodeid="48186">而逻辑回归虽然名字里有“回归”二字，但其实是一个分类模型，它是希望用一根线去把两波样本尽可能分开。</p>
<ul data-nodeid="48187">
<li data-nodeid="48188">
<p data-nodeid="48189"><strong data-nodeid="48628">从表达式上看</strong></p>
</li>
</ul>
<p data-nodeid="48190">逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数的映射，在最终的类别判断上，还需要对比一下预测值和 0.5 之间的大小关系。</p>
<p data-nodeid="48191">因此，线性回归解决的是回归问题，输出的连续值；而逻辑回归解决的是二分类问题，输出的是“0”或“1”的离散值。</p>
<ul data-nodeid="48192">
<li data-nodeid="48193">
<p data-nodeid="48194"><strong data-nodeid="48634">从机理上看</strong></p>
</li>
</ul>
<p data-nodeid="48195">逻辑回归增加了 sigmoid 函数，可以让预测结果在 0.5 附近产生更大的变化率，变化率更大，意味着梯度更大。</p>
<p data-nodeid="48196">在使用梯度下降法的时候，这样的机理，让模型的预测值会倾向于离开变化率大的地方，而收敛在“0”或“1”附近。这样的模型机理，会让它更适合用于分类问题的建模，具有更好的鲁棒性。</p>
<h3 data-nodeid="48197">小结</h3>
<p data-nodeid="48198"><strong data-nodeid="48648">逻辑回归</strong>是人工智能领域中分类问题的入门级算法。利用 AI 基本框架来看，它的 3 个核心公式分别是<br>
<img src="https://s0.lgstatic.com/i/image/M00/8C/30/Ciqc1F_pRgWAY4ynAABjhuo5U0E756.png" alt="WechatIMG1406.png" data-nodeid="48645"><br>
逻辑回归是个线性模型，具有计算简单、可解释性强等优势。它的不足是，只能处理线性问题，对于非线性问题则束手无策。</p>
<p data-nodeid="48199">最后，我们留一个思考题。试着把本课时中的代码，由随机梯度下降法改写为梯度下降法，再来求解一次参数 <i><strong data-nodeid="48656">w</strong></i> 吧。原则上除了计算量会变大以外，对分类结果是不会产生改变的。不妨亲自试一下。</p>
<p data-nodeid="49254">下一课时，我将向你讲解《20 | 决策树：如何对 NP 难复杂问题进行启发式求解？》</p>
<hr data-nodeid="49255">
<p data-nodeid="49256" class="te-preview-highlight"><a href="https://wj.qq.com/s2/7812549/4cd8/" data-nodeid="49262">课程评价入口，挑选 5 名小伙伴赠送小礼品～</a></p></div>

</body></html>