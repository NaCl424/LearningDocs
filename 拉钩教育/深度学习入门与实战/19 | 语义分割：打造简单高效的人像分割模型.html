<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>19 | 语义分割：打造简单高效的人像分割模型</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>19 | 语义分割：打造简单高效的人像分割模型</h1><p data-nodeid="84205" class="">上一讲，我向你介绍了语义分割的原理。在理解上一课时中 U-Net 语义分割网络的基础上，这一讲，让我们来实际构建一个人像分割模型吧。</p>
<h3 data-nodeid="84206">语义分割的评估</h3>
<p data-nodeid="84207">我们先简单回顾一下语义分割的目的：<strong data-nodeid="84394">把一张图中的每一个像素进行预测，并将它们分成对应的类别</strong>。如下图所示：</p>
<p data-nodeid="84208"><img src="https://s0.lgstatic.com/i/image/M00/8B/F8/CgqCHl_i6DyAFvUuAALm9LdMnm4760.png" alt="Drawing 0.png" data-nodeid="84397"></p>
<div data-nodeid="84209"><p style="text-align:center">图 1：图像分割举例</p></div>
<p data-nodeid="84210">你可能注意到，在图中有一个新的概念<strong data-nodeid="84403">Ground Truth</strong>（GT），它在目标检测与语义分割项目中十分常见。其实 GT 实际的表现形式就是我们下面会讲到的 mask。</p>
<p data-nodeid="84211">GT 也就是真实的类别、真实的结果，在语义分割中指每个像素真实的类别，这些类别是我们人工标记好的。所以在语义分割任务中，我们希望模型的输出（Prediction），能够尽可能高精度地与 GT 重合。</p>
<p data-nodeid="84212">那如何衡量这种重合程度呢？</p>
<p data-nodeid="84213">在图像分割中，我们会采用一个新的指标来衡量模型分割的好坏，我们把它叫作 IoU（Intersection over Union，交并比），用来衡量重合程度的高低。</p>
<p data-nodeid="84214">关于 IoU 的计算过程也很简单，我们来看一个例子：</p>
<pre class="lang-js" data-nodeid="84215"><code data-language="js"># 为了易于理解，我们先定义几个方法
def count(area):
    """
    返回给定区域元素的个数
    """
    return area 的元素个数
def intersection(area1, area2):
    """
    返回 area1 与 area2 的交集
    """
    return area1 与 area2 的交集
def union(area1, area2):
    """
    返回 area1 与 area2 的并集
    """
    return area1 与 area2 的并集

IoU = intersection(GT, Prediction) / union(GT, Prediction)
</code></pre>
<p data-nodeid="84216">案例中 IoU 的取值范围是 0~1，其中 0 代表预测部分完全没有覆盖到真实区域，1 代表我们的预测完美地覆盖到了真实区域。</p>
<p data-nodeid="84217">利用下面的例子，我们来可视化地了解一下 IoU 是如何计算。下图左边是 GT，右边是我们模型的预测结果。</p>
<p data-nodeid="84218"><img src="https://s0.lgstatic.com/i/image/M00/8B/ED/Ciqc1F_i6EuARsaSAATSba4X6jw439.png" alt="Drawing 1.png" data-nodeid="84414"></p>
<div data-nodeid="84219"><p style="text-align:center">图 2：GT 与 Prediction</p></div>
<p data-nodeid="84220">分子是 GT 与预测结果的交集（下图左边的图片），分母是 GT 与预测结果的并集（下面右边的图片），用分子除以分母就是我们需要的 IoU，即 Intersection / Union。</p>
<p data-nodeid="84221"><img src="https://s0.lgstatic.com/i/image/M00/8B/F8/CgqCHl_i6FKAftVIAATTto6J2vo974.png" alt="Drawing 2.png" data-nodeid="84418"></p>
<div data-nodeid="84222"><p style="text-align:center">图 3： GT 与 Prediction的交集与并集</p></div>
<p data-nodeid="84223">我们可以使用 NumPy 编写一段 IoU 的计算程序：</p>
<pre class="lang-java" data-nodeid="84224"><code data-language="java">intersection = np.logical_and(target, prediction)
union = np.logical_or(target, prediction)
iou_score = np.sum(intersection) / np.sum(union)
</code></pre>
<p data-nodeid="84225">在评价过程中，每个类别都会有一个 IoU。IoU 是语义分割中经常采用的一种评价方式，无论是论文还是各种比赛里都会看到。也许你会好奇，为什么不使用像素的精确度来衡量语义分割模型呢？</p>
<p data-nodeid="84226">其实也不是不可以，只不过在类别严重不平衡的时候，像素的精确度会严重误导我们，无法客观有效地评价模型。</p>
<p data-nodeid="84227">假设有如下场景：</p>
<ol data-nodeid="84228">
<li data-nodeid="84229">
<p data-nodeid="84230">一张图片有 100 个像素；</p>
</li>
<li data-nodeid="84231">
<p data-nodeid="84232">只有背景与类别 A，且类别 A 的 GT 的像素个数为 5；</p>
</li>
<li data-nodeid="84233">
<p data-nodeid="84234">模型将整张图片都预测为背景。</p>
</li>
</ol>
<p data-nodeid="84235">具体如下图所示：</p>
<p data-nodeid="84236"><img src="https://s0.lgstatic.com/i/image2/M01/03/D1/CgpVE1_i6GSARI31AAA9es3K9Jc948.png" alt="Drawing 3.png" data-nodeid="84429"></p>
<div data-nodeid="84237"><p style="text-align:center">图 4：整张图片都被预测为背景</p></div>
<p data-nodeid="84238">在这个假设的场景下，模型将整张图片都预测为背景，也就是在这张图中，模型有 5 个像素预测错了，那此时模型像素的精确度为：</p>
<pre class="lang-java" data-nodeid="84239"><code data-language="java">accuracy = <span class="hljs-number">95</span> / <span class="hljs-number">100</span> = <span class="hljs-number">0.95</span>
</code></pre>
<p data-nodeid="84240">显然，在这个类别严重不平衡的场景下，用 95%的精准度来衡量我们的模型是不合理的。</p>
<p data-nodeid="84241">我们希望模型能更好地分割出类别 A。但在这个场景中，我们的模型没有做到这一点，同时还有一个很高的精确度，这个精确度并不能反映出模型的实际情况。</p>
<p data-nodeid="84242">为了避免这种情况，有时候我们会使用所有类别的 IoU 的均值来综合衡量当前模型，我们把这个均值称为<strong data-nodeid="84442">mean IoU</strong>，简称<strong data-nodeid="84443">mIoU</strong>。mIoU 会通过所有类别的 IoU 的均值来综合衡量当前模型，这样得到的数据也能更准确地反映模型的实际情况。</p>
<p data-nodeid="84243">我们来看类别 A 的 IoU 以及模型的 mIoU 分别是多少。</p>
<pre class="lang-java" data-nodeid="84244"><code data-language="java">类别 A 的 IoU：
IoU_A = intersection(GT_A, Prediction_A) / union(GT_A, Prediction_A)
= <span class="hljs-number">0</span> / (<span class="hljs-number">5</span> + <span class="hljs-number">0</span> - <span class="hljs-number">0</span>) = <span class="hljs-number">0</span>
背景的 IoU：
IoU_BG = intersection(GT_BG, Prediction_BG) / union(GT_BG, Prediction_BG)
= <span class="hljs-number">95</span> / (<span class="hljs-number">95</span> + <span class="hljs-number">100</span> - <span class="hljs-number">95</span>) = <span class="hljs-number">0.95</span>
mIoU = (IoU_A + IoU_BG) / <span class="hljs-number">2</span> = (<span class="hljs-number">0</span> + <span class="hljs-number">0.95</span>) = <span class="hljs-number">0.475</span>
</code></pre>
<p data-nodeid="84245">通过计算可以发现，类别 A 的 IoU 是 0%，模型的 mIoU 是 47.5%。</p>
<p data-nodeid="84246">虽然模型的性能依然很差，无法很好地分割类别 A，但它的评估指标也不是很高。相比 95%的精确度，mIoU 的 47.5% 显然更加客观、合理。你可以根据 mIoU 反映的情况查看是哪里出了问题，然后进一步提升模型的性能。</p>
<p data-nodeid="84247">从这一简单的场景中能很明显地看到，在数据量严重不平衡的时候，像素的精确度是无法衡量我们的模型的。模型如果将所有的像素都预测为背景，模型的精确度再高都没有用。所以，mIoU 是我们在语义分割中经常使用的一个指标。</p>
<h3 data-nodeid="84248">语义分割网络的训练</h3>
<p data-nodeid="84249">在介绍“如何训练语义分割网络”前，我要引入一个概念<strong data-nodeid="84454">mask 图片</strong>。mask 图片一般是一张单通道的图片，里面的数值标记对应图片在相同位置的像素类别。</p>
<p data-nodeid="84250">以人像分割举例，我们的目的是把一张图片中的人像与背景分割出来。那么人像的像素在 mask 中对应的位置就是 1，背景就是 0。</p>
<p data-nodeid="84251">1 代表人像这个类别，0 代表背景这个类别。</p>
<p data-nodeid="84252">当然，根据项目的不同，数值是可以任意取的。</p>
<p data-nodeid="84253">刚才讲到的 GT 与语义分割模型的最终的输出都是一个 mask。不过对于 GT 的 mask，我们经常会省略 mask，直接叫 GT。</p>
<p data-nodeid="84254">在训练语义分割模型的时候，训练数据一般包含 2 个文件夹。</p>
<ol data-nodeid="84255">
<li data-nodeid="84256">
<p data-nodeid="84257"><strong data-nodeid="84464">JPEGImages</strong>文件夹：存放原图。</p>
</li>
<li data-nodeid="84258">
<p data-nodeid="84259"><strong data-nodeid="84469">SementationClass</strong>文件夹：存放 GT 的 mask 图片。</p>
</li>
</ol>
<p data-nodeid="84260">以上 2 个文件夹的名字并没有硬性要求，举例的只是较为常用的名字。</p>
<p data-nodeid="84261">以下 2 张图片分别是原图与 mask 图片：</p>
<p data-nodeid="84262"><img src="https://s0.lgstatic.com/i/image2/M01/03/D0/Cip5yF_i6ICAd44pABmgJ8cjFXU931.png" alt="Drawing 4.png" data-nodeid="84474"><br>
<img src="https://s0.lgstatic.com/i/image2/M01/03/D1/CgpVE1_i6ISACMOMAAA80lrG1c8858.png" alt="Drawing 5.png" data-nodeid="84478"></p>
<div data-nodeid="84263"><p style="text-align:center">图 5：原图和 mask 图片</p></div>
<p data-nodeid="84264">为什么需要 mask 图片呢？</p>
<p data-nodeid="84265">机器学习或者说深度学习的目标就是让机器像人一样思考，但机器怎么像人一样思考呢？答案是人来教机器如何思考。</p>
<p data-nodeid="84266">在《<a href="https://kaiwu.lagou.com/course/courseInfo.htm?courseId=522#/detail/pc?id=4990" data-nodeid="84486">16 | 图像分类：技术背景与常用模型解析</a><strong data-nodeid="84508">》<strong data-nodeid="84507">中，我们学习了图像分类模型的训练方式。图像分类模型最初所有的参数都是随机初始化的，所以我们要把训练数据中的图片</strong>提前分好类</strong>，然后在训练时，通过不断地<strong data-nodeid="84509">前向</strong>/<strong data-nodeid="84510">后向传播来降低 loss</strong>（也就是让我们预测的类别无限的接近真实的类别），从而<strong data-nodeid="84511">更新网络参数</strong>。这个真实的类别需要我们事先人工分好。</p>
<p data-nodeid="84267">例如我们要训练一个模型来分类猫和狗，那么在训练前，我们要把狗的图片放到 dog 文件夹里，猫的图片放到 cat 文件夹里。在训练的时候，模型会读取对应类别文件夹的数据进行训练。</p>
<p data-nodeid="84268"><img src="https://s0.lgstatic.com/i/image2/M01/03/D1/CgpVE1_i6JGAEd2BAAE7sleh4hM699.png" alt="Drawing 6.png" data-nodeid="84515"></p>
<div data-nodeid="84269"><p style="text-align:center">图 6：cat 与 dog 文件夹</p></div>
<p data-nodeid="84270">那么在语义分割中，应该如何训练呢？</p>
<p data-nodeid="84271">语义分割的目的是把每一个像素都分成相应的类别，但是我们又怎么把图像中的像素，像图片那样提前分好类别呢？</p>
<p data-nodeid="84272">在语义分割中我们首先要定义模型的目标，即 GT，<strong data-nodeid="84523">告诉模型每张图片每个像素所属的类别</strong>。在训练过程中，模型会不断更新参数来减小 loss。这样，我们就可以获得一个语义分割的模型了。</p>
<h4 data-nodeid="84273">图片标记工具</h4>
<p data-nodeid="84274">在这里我要向你介绍一下如何标记语义分割的数据（图片），生成 GT。</p>
<p data-nodeid="84275">好的数据集是获得一个高精度模型的第一步，尤其是在我们的语义分割任务中。现在有很多公开的数据集可以用，例如 COCO Person、VOC Person、Supervisely。但是这些公开数据中，有些数据可能并不适合我们的场景，并且标记的质量不是很高。</p>
<p data-nodeid="84276">我们可以下载开源的人像数据集：<a href="https://supervise.ly/explore/projects/supervisely-person-dataset-23304/datasets" data-nodeid="84530">Supervisely Dataset</a>（需要自己注册账号）与 <a href="https://github.com/aisegmentcn/matting_human_datasets" data-nodeid="84534">AISegment</a>。你可以分别点击链接获取。</p>
<ul data-nodeid="84277">
<li data-nodeid="84278">
<p data-nodeid="84279"><strong data-nodeid="84540">Supervisely Dataset</strong>：一共有 5711 张图片，图片标记质量高，包含多人的图片。</p>
</li>
<li data-nodeid="84280">
<p data-nodeid="84281"><strong data-nodeid="84545">AISegment</strong>：目前最大的公开数据集，包含 34427 张高质量的标注图片。用这部分数据训练的模型已经投入商用。</p>
</li>
</ul>
<p data-nodeid="84282">为了提高模型的<strong data-nodeid="84555">健壮性</strong>与<strong data-nodeid="84556">精度</strong>，我们经常需要收集一些符合我们应用场景的图片，然后将其标记。</p>
<p data-nodeid="84283">我们标记的工具叫作<strong data-nodeid="84566">Labelme</strong>，安装地址为<a href="https://github.com/wkentaro/labelme" data-nodeid="84564">https://github.com/wkentaro/labelme</a>。</p>
<p data-nodeid="84284">标记共分为 6 个步骤。</p>
<p data-nodeid="84285">（1）将需要标记的图片放入一个文件夹。</p>
<p data-nodeid="84286">（2）准备一个 labels.txt 文件，内容是需要标记类别的名字。</p>
<p data-nodeid="84287">因为是人像分割任务，所以 labels.txt 的内容是_background_与 person 就可以了，每一类占用一行，如下所示：</p>
<pre class="lang-java" data-nodeid="84288"><code data-language="java">_background_
person
</code></pre>
<p data-nodeid="84289">（3）在命令行中输入以下代码：</p>
<pre class="lang-java" data-nodeid="84290"><code data-language="java">labelme --labels labels.txt --nodata
</code></pre>
<p data-nodeid="84291">然后会出现一个界面：</p>
<p data-nodeid="84292"><img src="https://s0.lgstatic.com/i/image/M00/8B/ED/Ciqc1F_i6L6ANoVhAAHMoN88zXE433.png" alt="Drawing 7.png" data-nodeid="84579"></p>
<div data-nodeid="84293"><p style="text-align:center">图 7：标记界面</p></div>
<p data-nodeid="84294">（4）点击 open dir，选择第一步中的文件夹，将需要标记的图片导入进来。</p>
<p data-nodeid="84295"><img src="https://s0.lgstatic.com/i/image/M00/8B/F8/CgqCHl_i6MWAe8TqABuQWuK2J8U701.png" alt="Drawing 8.png" data-nodeid="84583"></p>
<div data-nodeid="84296"><p style="text-align:center">图 8：需要标记的原图</p></div>
<p data-nodeid="84297">（5）点击 Create Polygons，把需要标记的每一类都框起来。</p>
<p data-nodeid="84298">在人像分割任务中，你需要把图中的人物沿着人物的轮廓标记出来。需要注意的是，标记的时候起点和终点必须是同一个地方。在标记完成之后，从弹出的对话框中选择对应的类别。如图所示：</p>
<p data-nodeid="84299"><img src="https://s0.lgstatic.com/i/image/M00/8B/ED/Ciqc1F_i6M-AGITAABrTvwnXG4g046.png" alt="Drawing 9.png" data-nodeid="84588"></p>
<div data-nodeid="84300"><p style="text-align:center">图 9：标记图片</p></div>
<p data-nodeid="84301">每编辑完一张图，点击 save，就会自动生成一张同名的 json 文件。</p>
<p data-nodeid="84302">（6）在命令行中执行下面的命令，就可以将标记好的图片转换为一张 mask 图片。</p>
<pre class="lang-java" data-nodeid="84303"><code data-language="java">labelme_json_to_dataset json 文件名
</code></pre>
<p data-nodeid="84304">执行完上述的命令之后，会生成下面的文件，其中 label.png 就是我们所需要的 GT 的 mask。</p>
<p data-nodeid="84305"><img src="https://s0.lgstatic.com/i/image2/M01/03/D0/Cip5yF_i6NiATciTAAnKmKUI8UU348.png" alt="Drawing 10.png" data-nodeid="84594"></p>
<div data-nodeid="84306"><p style="text-align:center">图 10：标记完成</p></div>
<p data-nodeid="84307">到这里，咱们的标记就算完成了。</p>
<h3 data-nodeid="84308">视频虚拟背景：人像分割</h3>
<p data-nodeid="84309">这篇文章写于 2020 年，这一年对所有人来说都是不平凡的一年。突如其来的疫情席卷了全球，在家办公成了常态。在家使用视频会议时，你肯定接触过虚拟背景这个功能。</p>
<p data-nodeid="84310">所谓人像分割模型，就是把一张图片作为输入，有人的部分作为前景输出。一般，人的部分作为类别 1 输出，而背景则作为类别 0 输出。</p>
<p data-nodeid="84311">接下来，我们就要具体来实现一个 U-Net，用它来训练一个人像分割模型了。我会从以下 4 个方面进行介绍：数据加载、网络结构定义、层的定义、损失函数。</p>
<p data-nodeid="84312">为什么主要介绍这 4 个方面呢？因为机器学习任务离不开<strong data-nodeid="84613">数据加载</strong>、<strong data-nodeid="84614">计算 loss</strong>、<strong data-nodeid="84615">更新权重</strong>这三大块。其中，权重就是网络结构中的参数，网络结构中的参数又体现于 U-Net 的网络结构与层的定义。</p>
<h4 data-nodeid="84313">数据加载</h4>
<p data-nodeid="84314">在我们的人像分割项目中，我们要把原图（JPEGImages）的数据读取到一个 NumPy 的数组中，GT（SegementationClass）的数据读取到另一个 NumPy 的数组中，这一过程就是<strong data-nodeid="84622">数据加载</strong>。</p>
<p data-nodeid="84315">我们先创建一个工作目录，叫 u-net-dev。在 u-net-dev 下面创建一个 data_set 文件夹，用来存放我们的数据。</p>
<p data-nodeid="84316">我从 VOC 数据集中把和人有关的图片都挑选出来了，把数据加载成 NumPy 的数组，提供给模型训练。</p>
<p data-nodeid="84317"><img src="https://s0.lgstatic.com/i/image2/M01/03/D0/Cip5yF_i6OGAPg4NAAI3jPVPR6c390.png" alt="Drawing 11.png" data-nodeid="84629"></p>
<div data-nodeid="84318"><p style="text-align:center">图 11：界面展示</p></div>
<p data-nodeid="84319"><strong data-nodeid="84634">JPEGImages 文件夹用于存放原图，SegmentationClass 文件夹用于存放对应的 GT</strong>。</p>
<p data-nodeid="84320">在工作目录下面创建一个 utils 文件夹，用来存放与本项目有关的一切工具代码，包括 loader.py 和 dataset.py。</p>
<ul data-nodeid="84321">
<li data-nodeid="84322">
<p data-nodeid="84323">loader.py：所有和数据相关的代码都会写在 loader.py 文件中。</p>
</li>
<li data-nodeid="84324">
<p data-nodeid="84325">dataset.py：定义一个 DataSet 类别，用来抽象我们的数据。</p>
</li>
</ul>
<p data-nodeid="84326">完成上面 2 个 py 文件之后，我们只需要在训练时通过下面的代码即可导入训练数据：</p>
<pre class="lang-java" data-nodeid="84327"><code data-language="java"><span class="hljs-function">from util <span class="hljs-keyword">import</span> loader as data_loader
def <span class="hljs-title">load_dataset</span><span class="hljs-params">(train_rate)</span>:
    """
&nbsp; &nbsp; 按比例将数据分割为训练数据与评估数据

    Args:
&nbsp; &nbsp; train_rate: 训练数据所占的比例
&nbsp; &nbsp; Returns:
&nbsp; &nbsp; &nbsp; &nbsp; 训练集、测试集
&nbsp; &nbsp; """
&nbsp; &nbsp; loader </span>= data_loader.Loader(dir_original=<span class="hljs-string">"data_set/JPEGImages"</span>,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dir_segmented=<span class="hljs-string">"data_set/SegmentationClass"</span>)
&nbsp; &nbsp; <span class="hljs-keyword">return</span> loader.load_train_test(train_rate=train_rate, shuffle=False)
train, test = load_dataset(train_rate=parser.trainrate)
</code></pre>
<p data-nodeid="84328">具体的 <a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/loader.py" data-nodeid="84642">loader.py</a> 与 <a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/dataset.py" data-nodeid="84646">dataset.py</a> 的定义，因为代码过长，我将其放在了 GitHub 中，你可以点击相应的链接，我已经写好了注释。</p>
<h4 data-nodeid="84329">U-Net 网络结构</h4>
<p data-nodeid="84330">U-Net 网络结构是我们的重中之重，有了它模型可以知道以何种方式进行前向传播，分割图片，推断出图片中的前景（人像）与背景。我在《<strong data-nodeid="84654">18｜语义分割：技术背景与算法剖析</strong>》向你介绍了 U-Net 的运作原理，这里我们就来看 U-Net 的网络结构。</p>
<p data-nodeid="84331">我把 U-Net 的网络定义命名为 unet.py，也放在 util 文件夹中。</p>
<p data-nodeid="84332">这里我定义的是 1 个输入输出大小均为 256x256 的 U-Net，网络结构和参数与我在“18 课时”中介绍一样。</p>
<p data-nodeid="84333">有关 <a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/unet.py" data-nodeid="84660">unet.py</a> 的定义，你可以点击<a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/unet.py" data-nodeid="84664">链接</a>查看，我已经写好注释了。</p>
<h4 data-nodeid="84334">层的定义</h4>
<p data-nodeid="84335">完成 U-Net 的网络定义之后，我们就算是搭建好了一个模型的主要框架。这个框架中的细节该如何实现呢？方法就是层的定义。接下来我们看一下 U-Net 使用到层的具体实现。</p>
<p data-nodeid="84336">我定义了 U-Net 网络中使用到的一些层与操作。</p>
<ul data-nodeid="84337">
<li data-nodeid="84338">
<p data-nodeid="84339">conv：卷积层。</p>
</li>
<li data-nodeid="84340">
<p data-nodeid="84341">pooling：池化层。</p>
</li>
<li data-nodeid="84342">
<p data-nodeid="84343">up_conv：上采样层。</p>
</li>
<li data-nodeid="84344">
<p data-nodeid="84345">copy_and_crop： 连接操作。</p>
</li>
</ul>
<p data-nodeid="84346">我们把这些层与操作写在 layers.py 文件中，也把它放在 util 中。</p>
<p data-nodeid="84347">有关 <a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/layers.py" data-nodeid="84683">layers.py</a> 的定义，我同样放在了 GitHub 中，你可以点击<a href="https://github.com/deeplearningwithscholartree/codes/blob/master/lesson-20/layers.py" data-nodeid="84687">链接</a>查看，我已经写好注释了。</p>
<h4 data-nodeid="84348">损失函数</h4>
<p data-nodeid="84349">我们的人像分割问题也是分类问题，所以可以使用常见的交叉熵损失函数，一般有 Sigmoid 交叉熵损失函数与 Softmax 交叉熵损失函数：</p>
<ul data-nodeid="84350">
<li data-nodeid="84351">
<p data-nodeid="84352">Sigmoid 函数可以把一个数值映射到(0, 1)的区间内，所以经常用在二分类任务中；</p>
</li>
<li data-nodeid="84353">
<p data-nodeid="84354">Softmax 函数可以把一组数中的每个数映射到（0, 1）的区间内，并且和是 1，所以 Softmax 函数经常用于多分类。</p>
</li>
</ul>
<p data-nodeid="84355">我们的人像分割模型是一个二分类任务，可以使用上述交叉熵损失函数的任意一种。</p>
<p data-nodeid="84356">接下来，我们就看看该如何定义我们的损失函数以及优化方法吧。我选择的是 Softmax 交叉熵损失函数。当然，你也可以换成 Sigmoid 交叉熵损失。具体代码如下：</p>
<pre class="lang-java" data-nodeid="84357"><code data-language="java"># 加载数据，刚才已经讲过了
def load_dataset(train_rate):
&nbsp; &nbsp; loader = ld.Loader(dir_original="data_set/VOCdevkit/person/JPEGImages",
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dir_segmented="data_set/VOCdevkit/person/SegmentationClass")
&nbsp; &nbsp; return loader.load_train_test(train_rate=train_rate)
def train():
&nbsp; &nbsp; # 加载数据
&nbsp; &nbsp; train, test = load_dataset(train_rate=parser.trainrate)
&nbsp; &nbsp; # 创建模型
&nbsp; &nbsp; model = U_Net()
&nbsp; &nbsp; # 使用 Softmax 交叉熵损失函数以及 Adam 优化方法
&nbsp; &nbsp; cross_entropy = tf.reduce_mean(tf.nn.Softmax_cross_entropy_with_logits(labels=model.labels, logits=model.predict))
&nbsp; &nbsp; update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
&nbsp; &nbsp; with tf.control_dependencies(update_ops):
&nbsp; &nbsp; &nbsp; &nbsp; train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)
</code></pre>
<p data-nodeid="84358">训练中最主要的几个环节我已经讲完了，我们来回顾一下训练流程，把它们串起来。</p>
<ol data-nodeid="84359">
<li data-nodeid="84360">
<p data-nodeid="84361">加载数据，生成训练集与验证集（参考 loader.py）。</p>
</li>
<li data-nodeid="84362">
<p data-nodeid="84363">设定 batch_size 与 epoch 数， 以 batch_size 为单位循环读取训练集。</p>
</li>
<li data-nodeid="84364">
<p data-nodeid="84365">利用每个 batch 的数据进行前向传播与反向传播，计算 loss，更新权重。</p>
<ol data-nodeid="84366">
<li data-nodeid="84367">
<p data-nodeid="84368">根据我们写好的 u-net 的网络结构进行前向传播，这里会用到 layers.py 中使用到的层。</p>
</li>
<li data-nodeid="84369">
<p data-nodeid="84370">loss 我们也已经定义好了。</p>
</li>
<li data-nodeid="84371">
<p data-nodeid="84372">反向传播与更新权重是由我们使用的框架（TensorFlow）来帮助我们完成。</p>
</li>
</ol>
</li>
<li data-nodeid="84373">
<p data-nodeid="84374">通过在《<a href="https://kaiwu.lagou.com/course/courseInfo.htm?courseId=522#/detail/pc?id=4989" data-nodeid="84711">15 | TensorBoard：实验统计分析助手</a>》中学习的 TensorBoard 观察每个 batch 或者每个 epoch 的 loss，检验我们模型训练是否有问题。</p>
</li>
<li data-nodeid="84375">
<p data-nodeid="84376">保存模型。</p>
</li>
<li data-nodeid="84377">
<p data-nodeid="84378">评估模型，计算 mIoU。</p>
</li>
</ol>
<p data-nodeid="84379">如果 mIoU 的结果越接近 1，那么你建立的人像分割模型就越好。</p>
<h3 data-nodeid="84380">结语</h3>
<p data-nodeid="84381">恭喜你，你已经可以自己建立一个人像分割模型了。训练模型是一个需要根据结果优化的过程，要想直接建立一个性能非常好的人像分割模型是很难的。</p>
<p data-nodeid="84382">那么，对于人像分割模型，你觉得从哪些方面着手，可以提高模型的精度呢？欢迎在留言区以及在交流群中分享你的问题和回答。</p>
<p data-nodeid="84383">下一讲，我将带你了解“文本分类”，这是我们要实现的最后一个应用场景了。</p>
<p data-nodeid="84723" class="te-preview-highlight"><img src="https://s0.lgstatic.com/i/image/M00/8B/FC/CgqCHl_jAO2ANs5hAAUVlrfemYQ643.png" alt="Lark20201223-163332.png" data-nodeid="84726"></p></div>

</body></html>