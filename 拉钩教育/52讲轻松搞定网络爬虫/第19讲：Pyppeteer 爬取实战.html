<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>第19讲：Pyppeteer 爬取实战</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>第19讲：Pyppeteer 爬取实战</h1><p data-nodeid="10255">在上一课时我们了解了 Pyppeteer 的基本用法，确实我们可以发现其相比 Selenium 有很多方便之处。</p>
<p data-nodeid="10256">本课时我们就来使用 Pyppeteer 针对之前的 Selenium 案例做一次改写，来体会一下二者的不同之处，同时也加强一下对 Pyppeteer 的理解和掌握情况。</p>
<h3 data-nodeid="10257">爬取目标</h3>
<p data-nodeid="10258">本课时我们要爬取的目标和之前是一样的，还是 Selenium 的那个案例，地址为：<a href="https://dynamic2.scrape.center/" data-nodeid="10354">https://dynamic2.scrape.center/</a>，如下图所示。<br>
<img src="https://s0.lgstatic.com/i/image3/M01/85/2E/Cgq2xl6NsnyAfz5HAAJisxSlYKw076.png" alt="" data-nodeid="10358"><br>
这个网站的每个详情页的 URL 都是带有加密参数的，同时 Ajax 接口也都有加密参数和时效性。具体的介绍可以看下 Selenium 课时。</p>
<h3 data-nodeid="10259">本节目标</h3>
<p data-nodeid="10260">爬取目标和那一节也是一样的：</p>
<ul data-nodeid="10261">
<li data-nodeid="10262">
<p data-nodeid="10263">遍历每一页列表页，然后获取每部电影详情页的 URL。</p>
</li>
<li data-nodeid="10264">
<p data-nodeid="10265">爬取每部电影的详情页，然后提取其名称、评分、类别、封面、简介等信息。</p>
</li>
<li data-nodeid="10266">
<p data-nodeid="10267">爬取到的数据存为 JSON 文件。</p>
</li>
</ul>
<p data-nodeid="10268">要求和之前也是一样的，只不过我们这里的实现就全用 Pyppeteer 来做了。</p>
<h3 data-nodeid="10269">准备工作</h3>
<p data-nodeid="10270">在本课时开始之前，我们需要做好如下准备工作：</p>
<ul data-nodeid="10271">
<li data-nodeid="10272">
<p data-nodeid="10273">安装好 Python （最低为 Python 3.6）版本，并能成功运行 Python 程序。</p>
</li>
<li data-nodeid="10274">
<p data-nodeid="10275">安装好 Pyppeteer 并能成功运行示例。</p>
</li>
</ul>
<p data-nodeid="10276">其他的浏览器、驱动配置就不需要了，这也是相比 Selenium 更加方便的地方。</p>
<p data-nodeid="10277">页面分析在这里就不多介绍了，还是列表页 + 详情页的结构，具体可以参考 Selenium 那一课时的内容。</p>
<h3 data-nodeid="10278">爬取列表页</h3>
<p data-nodeid="10279">首先我们先做一些准备工作，定义一些基础的配置，包括日志定义、变量等等并引入一些必要的包，代码如下：</p>
<pre class="lang-python" data-nodeid="10280"><code data-language="python"><span class="hljs-keyword">import</span>&nbsp;logging
logging.basicConfig(level=logging.INFO,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;format=<span class="hljs-string">'%(asctime)s&nbsp;-&nbsp;%(levelname)s:&nbsp;%(message)s'</span>)
INDEX_URL&nbsp;=&nbsp;<span class="hljs-string">'https://dynamic2.scrape.center/page/{page}'</span>
TIMEOUT&nbsp;=&nbsp;<span class="hljs-number">10</span>
TOTAL_PAGE&nbsp;=&nbsp;<span class="hljs-number">10</span>
WINDOW_WIDTH,&nbsp;WINDOW_HEIGHT&nbsp;=&nbsp;<span class="hljs-number">1366</span>,&nbsp;<span class="hljs-number">768</span>
HEADLESS&nbsp;=&nbsp;<span class="hljs-literal">False</span>
</code></pre>
<p data-nodeid="10281">这里大多数的配置和之前是一样的，不过这里我们额外定义了窗口的宽高信息，这里定义为 1366 x 768，你也可以随意指定适合自己屏幕的宽高信息。另外这里定义了一个变量 HEADLESS，用来指定是否启用 Pyppeteer 的无头模式，如果为 False，那么启动 Pyppeteer 的时候就会弹出一个 Chromium 浏览器窗口。</p>
<p data-nodeid="10282">接着我们再定义一个初始化 Pyppeteer 的方法，包括启动 Pyppeteer，新建一个页面选项卡，设置窗口大小等操作，代码实现如下：</p>
<pre class="lang-python" data-nodeid="10283"><code data-language="python"><span class="hljs-keyword">from</span>&nbsp;pyppeteer&nbsp;<span class="hljs-keyword">import</span>&nbsp;launch
browser,&nbsp;tab&nbsp;=&nbsp;<span class="hljs-literal">None</span>,&nbsp;<span class="hljs-literal">None</span>
<span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">init</span>():</span>
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">global</span>&nbsp;browser,&nbsp;tab
&nbsp;&nbsp;&nbsp;browser&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;launch(headless=HEADLESS,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;args=[<span class="hljs-string">'--disable-infobars'</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">f'--window-size=<span class="hljs-subst">{WINDOW_WIDTH}</span>,<span class="hljs-subst">{WINDOW_HEIGHT}</span>'</span>])
&nbsp;&nbsp;&nbsp;tab&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;browser.newPage()
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.setViewport({<span class="hljs-string">'width'</span>:&nbsp;WINDOW_WIDTH,&nbsp;<span class="hljs-string">'height'</span>:&nbsp;WINDOW_HEIGHT})
</code></pre>
<p data-nodeid="10284">在这里我们先声明了一个 browser 对象，代表 Pyppeteer 所用的浏览器对象，tab 代表新建的页面选项卡，这里把两项设置为全局变量，方便其他的方法调用。</p>
<p data-nodeid="10285">另外定义了一个 init 方法，调用了 Pyppeteer 的 launch 方法，传入了 headless 为 HEADLESS，将其设置为非无头模式，另外还通过 args 指定了隐藏提示条并设定了窗口的宽高。</p>
<p data-nodeid="10286">接下来我们像之前一样，定义一个通用的爬取方法，代码如下：</p>
<pre class="lang-python" data-nodeid="10287"><code data-language="python"><span class="hljs-keyword">from</span>&nbsp;pyppeteer.errors&nbsp;<span class="hljs-keyword">import</span>&nbsp;TimeoutError
<span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">scrape_page</span>(<span class="hljs-params">url,&nbsp;selector</span>):</span>
&nbsp;&nbsp;&nbsp;logging.info(<span class="hljs-string">'scraping&nbsp;%s'</span>,&nbsp;url)
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">try</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.goto(url)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.waitForSelector(selector,&nbsp;options={
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'timeout'</span>:&nbsp;TIMEOUT&nbsp;*&nbsp;<span class="hljs-number">1000</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;})
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">except</span>&nbsp;TimeoutError:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logging.error(<span class="hljs-string">'error&nbsp;occurred&nbsp;while&nbsp;scraping&nbsp;%s'</span>,&nbsp;url,&nbsp;exc_info=<span class="hljs-literal">True</span>)
</code></pre>
<p data-nodeid="10288">这里我们定义了一个 scrape_page 方法，它接收两个参数，一个是 url，代表要爬取的链接，使用 goto 方法调用即可；另外一个是 selector，即要等待渲染出的节点对应的 CSS 选择器，这里我们使用 waitForSelector 方法并传入了 selector，并通过 options 指定了最长等待时间。</p>
<p data-nodeid="10289">这样的话在运行时页面会首先访问这个 URL，然后等待某个符合 selector 的节点加载出来，最长等待 10 秒，如果 10 秒内加载出来了，那就接着往下执行，否则抛出异常，捕获 TimeoutError 并输出错误日志。</p>
<p data-nodeid="10290">接下来，我们就实现一下爬取列表页的方法，代码实现如下：</p>
<pre class="lang-python" data-nodeid="10291"><code data-language="python"><span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">scrape_index</span>(<span class="hljs-params">page</span>):</span>
&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;INDEX_URL.format(page=page)
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;scrape_page(url,&nbsp;<span class="hljs-string">'.item&nbsp;.name'</span>)
</code></pre>
<p data-nodeid="10292">这里我们定义了 scrape_index 方法来爬取页面，其接受一个参数 page，代表要爬取的页码，这里我们首先通过 INDEX_URL 构造了列表页的 URL，然后调用 scrape_page 方法传入了 url 和要等待加载的选择器。</p>
<p data-nodeid="10293">这里的选择器我们使用的是 .item .name，这就是列表页中每部电影的名称，如果这个加载出来了，那么就代表页面加载成功了，如图所示。<br>
<img src="https://s0.lgstatic.com/i/image3/M01/0C/18/Ciqah16NsnyAKbHhAAPy_Jk6bGI885.png" alt="" data-nodeid="10395"></p>
<p data-nodeid="10294">好，接下来我们可以再定义一个解析列表页的方法，提取出每部电影的详情页 URL，定义如下：</p>
<pre class="lang-python" data-nodeid="10295"><code data-language="python"><span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">parse_index</span>():</span>
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorAllEval(<span class="hljs-string">'.item&nbsp;.name'</span>,&nbsp;<span class="hljs-string">'nodes&nbsp;=&gt;&nbsp;nodes.map(node&nbsp;=&gt;&nbsp;node.href)'</span>)
</code></pre>
<p data-nodeid="10296">这里我们调用了 querySelectorAllEval 方法，它接收两个参数，第一个参数是 selector，代表要选择的节点对应的 CSS 选择器；第二个参数是 pageFunction，代表的是要执行的 JavaScript 方法，这里需要传入的是一段 JavaScript 字符串，整个方法的作用是选择 selector 对应的节点，然后对这些节点通过 pageFunction 定义的逻辑抽取出对应的结果并返回。</p>
<p data-nodeid="10297">所以这里第一个参数 selector 就传入电影名称对应的节点，其实是超链接 a 节点。由于提取结果有多个，所以这里 JavaScript 对应的 pageFunction 输入参数就是 nodes，输出结果是调用了 map 方法得到每个 node，然后调用 node 的 href 属性即可。这样返回结果就是当前列表页的所有电影的详情页 URL 组成的列表了。</p>
<p data-nodeid="10298">好，接下来我们来串联调用一下看看，代码实现如下：</p>
<pre class="lang-python" data-nodeid="10299"><code data-language="python"><span class="hljs-keyword">import</span>&nbsp;asyncio
<span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">main</span>():</span>
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;init()
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">try</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span>&nbsp;page&nbsp;<span class="hljs-keyword">in</span>&nbsp;range(<span class="hljs-number">1</span>,&nbsp;TOTAL_PAGE&nbsp;+&nbsp;<span class="hljs-number">1</span>):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;scrape_index(page)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;detail_urls&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;parse_index()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logging.info(<span class="hljs-string">'detail_urls&nbsp;%s'</span>,&nbsp;detail_urls)
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">finally</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;browser.close()
<span class="hljs-keyword">if</span>&nbsp;__name__&nbsp;==&nbsp;<span class="hljs-string">'__main__'</span>:
&nbsp;&nbsp;&nbsp;asyncio.get_event_loop().run_until_complete(main())
</code></pre>
<p data-nodeid="10300">这里我们定义了一个 mian 方法，将前面定义的几个方法串联调用了一下。首先调用了 init 方法，然后循环遍历页码，调用了 scrape_index 方法爬取了每一页列表页，接着我们调用了 parse_index 方法，从列表页中提取出详情页的每个 URL，然后输出结果。</p>
<p data-nodeid="10301">运行结果如下：</p>
<pre class="lang-python" data-nodeid="10302"><code data-language="python"><span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">13</span>:<span class="hljs-number">54</span>:<span class="hljs-number">28</span>,<span class="hljs-number">879</span>&nbsp;-&nbsp;INFO:&nbsp;scraping&nbsp;https://dynamic2.scrape.center/page/<span class="hljs-number">1</span>
<span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">13</span>:<span class="hljs-number">54</span>:<span class="hljs-number">31</span>,<span class="hljs-number">411</span>&nbsp;-&nbsp;INFO:&nbsp;detail_urls&nbsp;[<span class="hljs-string">'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx'</span>,&nbsp;...,
<span class="hljs-string">'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI5'</span>,&nbsp;<span class="hljs-string">'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMA=='</span>]
<span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">13</span>:<span class="hljs-number">54</span>:<span class="hljs-number">31</span>,<span class="hljs-number">411</span>&nbsp;-&nbsp;INFO:&nbsp;scraping&nbsp;https://dynamic2.scrape.center/page/<span class="hljs-number">2</span>
</code></pre>
<p data-nodeid="10303">由于内容较多，这里省略了部分内容。</p>
<p data-nodeid="10304">在这里可以看到，每一次的返回结果都会是当前列表页提取出来的所有详情页 URL 组成的列表，我们下一步就可以用这些 URL 来接着爬取了。</p>
<h3 data-nodeid="10305">爬取详情页</h3>
<p data-nodeid="10306">拿到详情页的 URL 之后，下一步就是爬取每一个详情页然后提取信息了，首先我们定义一个爬取详情页的方法，代码如下：</p>
<pre class="lang-python" data-nodeid="10307"><code data-language="python"><span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">scrape_detail</span>(<span class="hljs-params">url</span>):</span>
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;scrape_page(url,&nbsp;<span class="hljs-string">'h2'</span>)
</code></pre>
<p data-nodeid="10308">代码非常简单，就是直接调用了 scrape_page 方法，然后传入了要等待加载的节点的选择器，这里我们就直接用了 h2 了，对应的就是详情页的电影名称，如图所示。<br>
<img src="https://s0.lgstatic.com/i/image3/M01/85/2E/Cgq2xl6Nsn2ADDDkAAVLMGBzKmQ220.png" alt="" data-nodeid="10415"><br>
如果顺利运行，那么当前 Pyppeteer 就已经成功加载出详情页了，下一步就是提取里面的信息了。</p>
<p data-nodeid="10309">接下来我们再定义一个提取详情信息的方法，代码如下：</p>
<pre class="lang-python" data-nodeid="10310"><code data-language="python"><span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">parse_detail</span>():</span>
&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;tab.url
&nbsp;&nbsp;&nbsp;name&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorEval(<span class="hljs-string">'h2'</span>,&nbsp;<span class="hljs-string">'node&nbsp;=&gt;&nbsp;node.innerText'</span>)
&nbsp;&nbsp;&nbsp;categories&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorAllEval(<span class="hljs-string">'.categories&nbsp;button&nbsp;span'</span>,&nbsp;<span class="hljs-string">'nodes&nbsp;=&gt;&nbsp;nodes.map(node&nbsp;=&gt;&nbsp;node.innerText)'</span>)
&nbsp;&nbsp;&nbsp;cover&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorEval(<span class="hljs-string">'.cover'</span>,&nbsp;<span class="hljs-string">'node&nbsp;=&gt;&nbsp;node.src'</span>)
&nbsp;&nbsp;&nbsp;score&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorEval(<span class="hljs-string">'.score'</span>,&nbsp;<span class="hljs-string">'node&nbsp;=&gt;&nbsp;node.innerText'</span>)
&nbsp;&nbsp;&nbsp;drama&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;tab.querySelectorEval(<span class="hljs-string">'.drama&nbsp;p'</span>,&nbsp;<span class="hljs-string">'node&nbsp;=&gt;&nbsp;node.innerText'</span>)
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'url'</span>:&nbsp;url,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'name'</span>:&nbsp;name,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'categories'</span>:&nbsp;categories,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'cover'</span>:&nbsp;cover,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'score'</span>:&nbsp;score,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'drama'</span>:&nbsp;drama
&nbsp;&nbsp;&nbsp;}
</code></pre>
<p data-nodeid="10311">这里我们定义了一个 parse_detail 方法，提取了 URL、名称、类别、封面、分数、简介等内容，提取方式如下：</p>
<ul data-nodeid="10312">
<li data-nodeid="10313">
<p data-nodeid="10314">URL：直接调用 tab 对象的 url 属性即可获取当前页面的 URL。</p>
</li>
<li data-nodeid="10315">
<p data-nodeid="10316">名称：由于名称只有一个节点，所以这里我们调用了 querySelectorEval 方法来提取，而不是querySelectorAllEval，第一个参数传入 h2，提取到了名称对应的节点，然后第二个参数传入提取的 pageFunction，调用了 node 的 innerText 属性提取了文本值，即电影名称。</p>
</li>
<li data-nodeid="10317">
<p data-nodeid="10318">类别：类别有多个，所以我们这里调用了 querySelectorAllEval 方法来提取，其对应的 CSS 选择器为 .categories button span，可以选中多个类别节点。接下来还是像之前提取详情页 URL 一样，pageFunction 使用 nodes 参数，然后调用 map 方法提取 node 的 innerText 就得到所有类别结果了。</p>
</li>
<li data-nodeid="10319">
<p data-nodeid="10320">封面：同样地，可以使用 CSS 选择器 .cover 直接获取封面对应的节点，但是由于其封面的 URL 对应的是 src 这个属性，所以这里提取的是 src 属性。</p>
</li>
<li data-nodeid="10321">
<p data-nodeid="10322">分数：分数对应的 CSS 选择器为 .score ，类似的原理，提取 node 的 innerText 即可。</p>
</li>
<li data-nodeid="10323">
<p data-nodeid="10324">简介：同样可以使用 CSS 选择器 .drama p 直接获取简介对应的节点，然后调用 innerText 属性提取文本即可。</p>
</li>
</ul>
<p data-nodeid="10325">最后我们将提取结果汇总成一个字典然后返回即可。</p>
<p data-nodeid="10326">接下来 main 方法里面，我们增加 scrape_detail 和 parse_detail 方法的调用，main 方法改写如下：</p>
<pre class="lang-python" data-nodeid="10327"><code data-language="python"><span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">main</span>():</span>
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;init()
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">try</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span>&nbsp;page&nbsp;<span class="hljs-keyword">in</span>&nbsp;range(<span class="hljs-number">1</span>,&nbsp;TOTAL_PAGE&nbsp;+&nbsp;<span class="hljs-number">1</span>):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;scrape_index(page)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;detail_urls&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;parse_index()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span>&nbsp;detail_url&nbsp;<span class="hljs-keyword">in</span>&nbsp;detail_urls:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;scrape_detail(detail_url)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;detail_data&nbsp;=&nbsp;<span class="hljs-keyword">await</span>&nbsp;parse_detail()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logging.info(<span class="hljs-string">'data&nbsp;%s'</span>,&nbsp;detail_data)
&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">finally</span>:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">await</span>&nbsp;browser.close()
</code></pre>
<p data-nodeid="10328">重新看下运行结果，运行结果如下：</p>
<pre class="lang-python" data-nodeid="10329"><code data-language="python"><span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">14</span>:<span class="hljs-number">12</span>:<span class="hljs-number">39</span>,<span class="hljs-number">564</span>&nbsp;-&nbsp;INFO:&nbsp;scraping&nbsp;https://dynamic2.scrape.center/page/<span class="hljs-number">1</span>
<span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">14</span>:<span class="hljs-number">12</span>:<span class="hljs-number">42</span>,<span class="hljs-number">935</span>&nbsp;-&nbsp;INFO:&nbsp;scraping&nbsp;https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx
<span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">14</span>:<span class="hljs-number">12</span>:<span class="hljs-number">45</span>,<span class="hljs-number">781</span>&nbsp;-&nbsp;INFO:&nbsp;data&nbsp;{<span class="hljs-string">'url'</span>:&nbsp;<span class="hljs-string">'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx'</span>,&nbsp;<span class="hljs-string">'name'</span>:&nbsp;<span class="hljs-string">'霸王别姬&nbsp;-&nbsp;Farewell&nbsp;My&nbsp;Concubine'</span>,&nbsp;<span class="hljs-string">'categories'</span>:&nbsp;[<span class="hljs-string">'剧情'</span>,&nbsp;<span class="hljs-string">'爱情'</span>],&nbsp;<span class="hljs-string">'cover'</span>:&nbsp;<span class="hljs-string">'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd7896cf62472.jpg@464w_644h_1e_1c'</span>,&nbsp;<span class="hljs-string">'score'</span>:&nbsp;<span class="hljs-string">'9.5'</span>,&nbsp;<span class="hljs-string">'drama'</span>:&nbsp;<span class="hljs-string">'影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅&nbsp;饰）与程蝶衣（张国荣&nbsp;饰）是一对打小一起长大的师兄弟，两人一个演生，一个饰旦，一向配合天衣无缝，尤其一出《霸王别姬》，更是誉满京城，为此，两人约定合演一辈子《霸王别姬》。但两人对戏剧与人生关系的理解有本质不同，段小楼深知戏非人生，程蝶衣则是人戏不分。段小楼在认为该成家立业之时迎娶了名妓菊仙（巩俐&nbsp;饰），致使程蝶衣认定菊仙是可耻的第三者，使段小楼做了叛徒，自此，三人围绕一出《霸王别姬》生出的爱恨情仇战开始随着时代风云的变迁不断升级，终酿成悲剧。'</span>}
<span class="hljs-number">2020</span><span class="hljs-number">-04</span><span class="hljs-number">-08</span>&nbsp;<span class="hljs-number">14</span>:<span class="hljs-number">12</span>:<span class="hljs-number">45</span>,<span class="hljs-number">782</span>&nbsp;-&nbsp;INFO:&nbsp;scraping&nbsp;https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIy
</code></pre>
<p data-nodeid="10330">这里可以看到，首先先爬取了列表页，然后提取出了详情页之后接着开始爬详情页，然后提取出我们想要的电影信息之后，再接着去爬下一个详情页。</p>
<p data-nodeid="10331">这样，所有的详情页都会被我们爬取下来啦。</p>
<h3 data-nodeid="10332">数据存储</h3>
<p data-nodeid="10333">最后，我们再像之前一样添加一个数据存储的方法，为了方便，这里还是保存为 JSON 文本文件，实现如下：</p>
<pre class="lang-python" data-nodeid="10334"><code data-language="python"><span class="hljs-keyword">import</span>&nbsp;json
<span class="hljs-keyword">from</span>&nbsp;os&nbsp;<span class="hljs-keyword">import</span>&nbsp;makedirs
<span class="hljs-keyword">from</span>&nbsp;os.path&nbsp;<span class="hljs-keyword">import</span>&nbsp;exists
RESULTS_DIR&nbsp;=&nbsp;<span class="hljs-string">'results'</span>
exists(RESULTS_DIR)&nbsp;<span class="hljs-keyword">or</span>&nbsp;makedirs(RESULTS_DIR)
<span class="hljs-keyword">async</span>&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">save_data</span>(<span class="hljs-params">data</span>):</span>
&nbsp;&nbsp;&nbsp;name&nbsp;=&nbsp;data.get(<span class="hljs-string">'name'</span>)
&nbsp;&nbsp;&nbsp;data_path&nbsp;=&nbsp;<span class="hljs-string">f'<span class="hljs-subst">{RESULTS_DIR}</span>/<span class="hljs-subst">{name}</span>.json'</span>
&nbsp;&nbsp;&nbsp;json.dump(data,&nbsp;open(data_path,&nbsp;<span class="hljs-string">'w'</span>,&nbsp;encoding=<span class="hljs-string">'utf-8'</span>),&nbsp;ensure_ascii=<span class="hljs-literal">False</span>,&nbsp;indent=<span class="hljs-number">2</span>)
</code></pre>
<p data-nodeid="10335">这里原理和之前是完全相同的，但是由于这里我们使用的是 Pyppeteer，是异步调用，所以 save_data 方法前面需要加 async。</p>
<p data-nodeid="10336">最后添加上 save_data 的调用，完整看下运行效果。</p>
<h3 data-nodeid="10337">问题排查</h3>
<p data-nodeid="10338">在运行过程中，由于 Pyppeteer 本身实现的原因，可能连续运行 20 秒之后控制台就会出现如下错误：</p>
<pre class="lang-python" data-nodeid="10339"><code data-language="python">pyppeteer.errors.NetworkError:&nbsp;Protocol&nbsp;Error&nbsp;(Runtime.evaluate):&nbsp;Session&nbsp;closed.&nbsp;Most&nbsp;likely&nbsp;the&nbsp;page&nbsp;has&nbsp;been&nbsp;closed.
</code></pre>
<p data-nodeid="10340">其原因是 Pyppeteer 内部使用了 Websocket，在 Websocket 客户端发送 ping 信号 20 秒之后仍未收到 pong 应答，就会中断连接。</p>
<p data-nodeid="10341">问题的解决方法和详情描述见 <a href="https://github.com/miyakogi/pyppeteer/issues/178" data-nodeid="10451">https://github.com/miyakogi/pyppeteer/issues/178</a>，此时我们可以通过修改 Pyppeteer 源代码来解决这个问题，对应的代码修改见：<a href="https://github.com/miyakogi/pyppeteer/pull/160/files" data-nodeid="10455">https://github.com/miyakogi/pyppeteer/pull/160/files</a>，即把 connect 方法添加 ping_interval=None, ping_timeout=None 两个参数即可。</p>
<p data-nodeid="10342">另外也可以复写一下 Connection 的实现，其解决方案同样可以在&nbsp;<a href="https://github.com/miyakogi/pyppeteer/pull/160" data-nodeid="10464">https://github.com/miyakogi/pyppeteer/pull/160</a>&nbsp;找到，如 patch_pyppeteer 的定义。</p>
<h3 data-nodeid="10343">无头模式</h3>
<p data-nodeid="10344">最后如果代码能稳定运行了，我们可以将其改为无头模式，将 HEADLESS 修改为 True 即可，这样在运行的时候就不会弹出浏览器窗口了。</p>
<h3 data-nodeid="10345">总结</h3>
<p data-nodeid="10346">本课时我们通过实例来讲解了 Pyppeteer 爬取一个完整网站的过程，从而对 Pyppeteer 的使用有进一步的掌握。</p>
<p data-nodeid="10347" class="te-preview-highlight">本节代码：<a href="https://github.com/Python3WebSpider/ScrapeDynamic2" data-nodeid="10475">https://github.com/Python3WebSpider/ScrapeDynamic2</a>。</p></div>

</body></html>