<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>第32讲：标准化机器学习流程：ML pipeline</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>第32讲：标准化机器学习流程：ML pipeline</h1><p data-nodeid="413">在开始今天的课程前，我们先来讲解一下上节课的课后思考题。深度学习与机器学习的不同之处在于：</p>
<ul data-nodeid="414">
<li data-nodeid="415">
<p data-nodeid="416">数据量大小。深度学习通常需要更多的样本才能达到更好的效果，所以通常深度学习的训练时间更长。</p>
</li>
<li data-nodeid="417">
<p data-nodeid="418">硬件区别。深度学习算法通常涉及大量浮点运算，样本量也巨大，而 GPU 天然的海量流处理器架构非常适合并行计算，所以一般复杂的深度学习应用通常需要 GPU 的硬件架构。</p>
</li>
<li data-nodeid="419">
<p data-nodeid="420">特征选择。一般机器学习解决问题时，都需要专家指定或者先验知识来确定特征，如信用模型，这些特征在很大程度上影响了模型的准确性。</p>
</li>
<li data-nodeid="421">
<p data-nodeid="422">解决问题的方法。当使用传统机器学习方法解决问题时，经常采取化整为零、分别解决、再合并结果求解的策略。而深度学习是端到端的模型，输入训练数据，再直接输出最终结果，让深度神经网络自己学习如何提取关键特征，比如对一张有着多个目标的照片进行目标检测，需要识别出目标的类别，并指出图中所在位置。典型机器学习方法会将这个问题分为两步：目标检测与目标识别。首先，使用边框检测技术，扫描全图找到所有可能的对象，对这些对象使用目标识别算法，如支持向量机，识别出相关物体。深度学习方法则按照端到端的方式处理这个问题，比如通过卷积神经网络就能够实现目标的定位与识别，也就是将原始图像输入到卷积神经网络模型中，再直接输出图像中目标的位置和类别。</p>
</li>
<li data-nodeid="423">
<p data-nodeid="424">可解释性。同神经网络算法一样，深度学习模型很难进行解释，这也使深度学习算法无法应用于很多要求模型可解释的场景，如金融等。</p>
</li>
</ul>
<p data-nodeid="425">接下来我们开始讲解今天的内容，标准化机器学习流程：ML Pipeline。Spark MLlib 是 Spark 机器学习套件， 它的目的是成为大数据机器学习的最佳实践。为了简化机器学习过程并使其可扩展，Spark ML API 引入了 Pipelines API（管道），这类似于 Python 机器学习库 Scikit-Learn 中的 Pipeline，它采用了一系列 API 定义并标准化了上一课时中我们学习的机器学习工作流，包含了数据收集、预处理、特征抽取、特征选择、模型拟合、模型验证、模型评估等一系列阶段。例如，对文档进行分类时，也许会包含分词、特征抽取、训练分类模型，以及调优等过程。大多数机器学习库不是为分布式计算而设计的，也不提供 Pipeline 的创建与调优，而这就是 Spark ML PipeLines 要做的。</p>
<p data-nodeid="426">Spark ML Pipelines 就是对分布式机器学习过程进行模块化地抽象，这样可以使多个算法合并成一个 Pipeline 或者使工作流变得更加容易，下面是 Pipelines API 的关键概念：</p>
<ul data-nodeid="427">
<li data-nodeid="428">
<p data-nodeid="429">DataFrame：DataFrame 与 Spark SQL 中用到的 DataFrame 一样，是 Spark 的基础数据结构，贯穿了整个 Pipeline。它可以存储文本、特征向量、训练集以及测试集。除了常见的类型，DataFrame 还支持 Spark MLlib 特有的 Vector 类型。</p>
</li>
<li data-nodeid="430">
<p data-nodeid="431">Transformer：Transformer 对应了数据转换的过程，它接收一个 DataFrame，在它的作用下，会生成一个新的 DataFrame。在机器学习中，在涉及特征转换的过程中经常会用到它。Transformer也可以用于使训练完成后的模型将特征数据集（测试集）转换为带有预测结果的数据集的场景。Transformer 必须实现 transform() 方法。</p>
</li>
<li data-nodeid="432">
<p data-nodeid="433">Estimator：从上面&nbsp;Transformer&nbsp;的定义中可以得知， <strong data-nodeid="488">训练完成好的模型也是一个 Transformer</strong> ，所以 Estimator 包含了一个可以让数据集拟合出一个 Transformer 的算法。 Estimator 必须实现 fit() 方法。</p>
</li>
<li data-nodeid="434">
<p data-nodeid="435">Pipeline：一个 Pipeline 可以将多个 Transformer 和 Estimator 组装成一个特定的机器学习工作流。</p>
</li>
<li data-nodeid="436">
<p data-nodeid="437">Parameter：所有的 Estimator 和 Transformer 共用一套通用的 API 来指定参数。</p>
</li>
</ul>
<p data-nodeid="438">文档分类是一个在自然语言处理中非常常见的应用，如垃圾邮件监测、情感分析等。下面，我们将通过一个文档分类的例子来让读者对 Spark 的 Pipeline 有一个感性的理解。简单来说，任何文档分类应用都需要以下 4 步：</p>
<ol data-nodeid="439">
<li data-nodeid="440">
<p data-nodeid="441">将文档分词。</p>
</li>
<li data-nodeid="442">
<p data-nodeid="443">将分词的结果转换为词向量。</p>
</li>
<li data-nodeid="444">
<p data-nodeid="445">学习模型。</p>
</li>
<li data-nodeid="446">
<p data-nodeid="447">预测（是否为垃圾邮件或者正负情感）。</p>
</li>
</ol>
<p data-nodeid="448">比如在垃圾邮件监测中，我们需要通过邮件正文甄别出哪些是垃圾邮件。垃圾邮件的正文一般会是一段文字，如：“代开各种发票，手续费极低，请联系我。”但这样一段文字是无法直接应用于 Estimator 的，需要将其转换为特征向量。一般做法是用一个词典构建一个向量空间，其中每一个维度都是一个词，出现过的为 1 ，未出现的为 0 ，再根据文档中出现的词语的频数，用 TF-IDF 算法为词维度赋予权重。这样的话，每个文档就能被转换为一个等长的特征向量，如下：</p>
<p data-nodeid="449">(0, 0, …, 0.27, 0, 0, …, 0.1, 0) ，接着就可以用它来拟合模型并输出测试结果。</p>
<p data-nodeid="450">我们用一个流程图来表示整个过程，如下图所示： Tokenizer 和 HashingTF 为 Transformer，作用分别是分词和计算权重，训练出的模型也是 Transformer，用来生成测试结果；Estimator 采用的是逻辑回归算法（LR）；DS0-DS3 都是不同阶段输出的数据，这就是一个完整意义上的 Pipeline。</p>
<p data-nodeid="451"><img alt="1.png" src="https://s0.lgstatic.com/i/image/M00/3A/CE/Ciqc1F8igOiAGSXYAAEv2-wuZAs564.png" data-nodeid="501"></p>
<p data-nodeid="452">下面用代码实现整个 Pipeline，如下：</p>
<pre class="lang-scala" data-nodeid="994"><code data-language="scala"><span class="hljs-keyword">import</span>&nbsp;org.apache.spark.ml.{<span class="hljs-type">Pipeline</span>,&nbsp;<span class="hljs-type">PipelineModel</span>} 
<span class="hljs-keyword">import</span>&nbsp;org.apache.spark.ml.classification.<span class="hljs-type">LogisticRegression</span> 
<span class="hljs-keyword">import</span>&nbsp;org.apache.spark.ml.feature.{<span class="hljs-type">HashingTF</span>,&nbsp;<span class="hljs-type">Tokenizer</span>} 
<span class="hljs-keyword">import</span>&nbsp;org.apache.spark.ml.linalg.<span class="hljs-type">Vector</span> 
<span class="hljs-keyword">import</span>&nbsp;org.apache.spark.sql.<span class="hljs-type">Row</span> 
<span class="hljs-keyword">import</span>&nbsp;org.apache.spark.sql.<span class="hljs-type">SparkSession</span> 

<span class="hljs-class"><span class="hljs-keyword">object</span><span class="hljs-title">&nbsp;PipelineExample</span></span>{ 

&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span><span class="hljs-title">&nbsp;main</span></span>(args:&nbsp;<span class="hljs-type">Array</span>[<span class="hljs-type">String</span>&nbsp;]):&nbsp;<span class="hljs-type">Unit</span>&nbsp;=&nbsp;{ 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;spark&nbsp;=&nbsp;<span class="hljs-type">SparkSession</span> 
&nbsp;&nbsp;&nbsp;&nbsp;.builder 
&nbsp;&nbsp;&nbsp;&nbsp;.master(<span class="hljs-string">"local[2]"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.appName(<span class="hljs-string">"PipelineExample"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.getOrCreate() 
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">import</span>&nbsp;spark.implicits._ 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;准备训练数据，其中最后一列，就是该文档的标签，即是否为垃圾邮件 </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;training&nbsp;=&nbsp;spark.createDataFrame(<span class="hljs-type">Seq</span>( 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">0</span>L,&nbsp;<span class="hljs-string">"a&nbsp;b&nbsp;c&nbsp;d&nbsp;e&nbsp;spark"</span>,&nbsp;<span class="hljs-number">1.0</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">1</span>L,&nbsp;<span class="hljs-string">"b&nbsp;d"</span>,&nbsp;<span class="hljs-number">0.0</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">2</span>L,&nbsp;<span class="hljs-string">"spark&nbsp;f&nbsp;g&nbsp;h"</span>,&nbsp;<span class="hljs-number">1.0</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">3</span>L,&nbsp;<span class="hljs-string">"hadoop&nbsp;mapreduce"</span>,&nbsp;<span class="hljs-number">0.0</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;)).toDF(<span class="hljs-string">"id"</span>,&nbsp;<span class="hljs-string">"text"</span>,&nbsp;<span class="hljs-string">"label"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;配置整个Pipeline,&nbsp;由3个组件组成：tokenizer（Transformer）、hashingTF（Transformer） </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;和&nbsp;lr（Estimator） </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;tokenizer&nbsp;=&nbsp;<span class="hljs-keyword">new</span>&nbsp;<span class="hljs-type">Tokenizer</span>() 
&nbsp;&nbsp;&nbsp;&nbsp;.setInputCol(<span class="hljs-string">"text"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.setOutputCol(<span class="hljs-string">"words"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;hashingTF&nbsp;=&nbsp;<span class="hljs-keyword">new</span>&nbsp;<span class="hljs-type">HashingTF</span>() 
&nbsp;&nbsp;&nbsp;&nbsp;.setNumFeatures(<span class="hljs-number">1000</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.setInputCol(tokenizer.getOutputCol) 
&nbsp;&nbsp;&nbsp;&nbsp;.setOutputCol(<span class="hljs-string">"features"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;lr&nbsp;=&nbsp;<span class="hljs-keyword">new</span>&nbsp;<span class="hljs-type">LogisticRegression</span>() 
&nbsp;&nbsp;&nbsp;&nbsp;.setMaxIter(<span class="hljs-number">10</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.setRegParam(<span class="hljs-number">0.001</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;pipeline&nbsp;=&nbsp;<span class="hljs-keyword">new</span>&nbsp;<span class="hljs-type">Pipeline</span>() 
&nbsp;&nbsp;&nbsp;&nbsp;.setStages(<span class="hljs-type">Array</span>(tokenizer,&nbsp;hashingTF,&nbsp;lr)) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;拟合模型，得到结果 </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;model&nbsp;=&nbsp;pipeline.fit(training) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;将模型持久化 </span>
&nbsp;&nbsp;&nbsp;&nbsp;model.write.overwrite().save(<span class="hljs-string">"/tmp/spark-logistic-regression-model"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;将Pipeline持久化 </span>
&nbsp;&nbsp;&nbsp;&nbsp;pipeline.write.overwrite().save(<span class="hljs-string">"/tmp/unfit-lr-model"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;加载模型 </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;sameModel&nbsp;=&nbsp;<span class="hljs-type">PipelineModel</span>.load(<span class="hljs-string">"/tmp/spark-logistic-regression-model"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;准备无标签的测试集 </span>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">val</span>&nbsp;test&nbsp;=&nbsp;spark.createDataFrame(<span class="hljs-type">Seq</span>( 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">4</span>L,&nbsp;<span class="hljs-string">"spark&nbsp;i&nbsp;j&nbsp;k"</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">5</span>L,&nbsp;<span class="hljs-string">"l&nbsp;m&nbsp;n"</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">6</span>L,&nbsp;<span class="hljs-string">"spark&nbsp;hadoop&nbsp;spark"</span>), 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="hljs-number">7</span>L,&nbsp;<span class="hljs-string">"apache&nbsp;hadoop"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;)).toDF(<span class="hljs-string">"id"</span>,&nbsp;<span class="hljs-string">"text"</span>) 

&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">//&nbsp;用模型预测测试集，得到预测结果（标签） </span>
&nbsp;&nbsp;&nbsp;&nbsp;sameModel.transform(test) 
&nbsp;&nbsp;&nbsp;&nbsp;.select(<span class="hljs-string">"id"</span>,&nbsp;<span class="hljs-string">"text"</span>,&nbsp;<span class="hljs-string">"probability"</span>,&nbsp;<span class="hljs-string">"prediction"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;.collect() 
&nbsp;&nbsp;&nbsp;&nbsp;.foreach&nbsp;{&nbsp;<span class="hljs-keyword">case</span>&nbsp;<span class="hljs-type">Row</span>(id:&nbsp;<span class="hljs-type">Long</span>,&nbsp;text:&nbsp;<span class="hljs-type">String</span>,&nbsp;prob:&nbsp;<span class="hljs-type">Vector</span>,&nbsp;prediction:&nbsp;<span class="hljs-type">Double</span>)&nbsp;=&gt; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;println(<span class="hljs-string">s"(<span class="hljs-subst">$id</span>,&nbsp;<span class="hljs-subst">$text</span>)&nbsp;--&gt;&nbsp;prob=<span class="hljs-subst">$prob</span>,&nbsp;prediction=<span class="hljs-subst">$prediction</span>"</span>) 
&nbsp;&nbsp;&nbsp;&nbsp;} 
&nbsp;&nbsp;} 
}
</code></pre>
<p data-nodeid="995">这样就用 Spark 完整实现了一个机器学习的流程。从上面的代码中可以看出，这样的结构非常有利于复用 Transformer 与 Estimator 组件。</p>
<p data-nodeid="996">Spark MLlib（ML API）的算法包主要分为以下几个部分：</p>
<ul data-nodeid="997">
<li data-nodeid="998">
<p data-nodeid="999">特征抽取、转换与选择；</p>
</li>
<li data-nodeid="1000">
<p data-nodeid="1001">分类和回归；</p>
</li>
<li data-nodeid="1002">
<p data-nodeid="1003">聚类；</p>
</li>
<li data-nodeid="1004">
<p data-nodeid="1005">协同过滤；</p>
</li>
<li data-nodeid="1006">
<p data-nodeid="1007">频繁项集挖掘。</p>
</li>
</ul>
<p data-nodeid="1008">其中每一类都有若干种算法的实现，用户可以利用 Pipeline 按需进行切换，下面我们将根据这几个类别，分别实现一些真实数据的案例，让读者可以直接上手应用。</p>
<p data-nodeid="1009">此外，在上面的代码中，我们用 Pipeline API 将模型序列化成文件，这样的好处在于可以将模型看成一个黑盒，非常方便模型上线，而不用在上线应用时再去对模型进行硬编码，这类似于 Python 的 Pickle 库的用法。</p>
<h3 data-nodeid="1010">小结</h3>
<p data-nodeid="1011">在 Spark 的早期版本中，并没有 ML pipeline API，这导致代码难以维护、可读性较差、也一定程度上影响了 MLlib 的流行。而 Pipeline API 的引入抽象了机器学习流程，让整个代码变得简洁优美，另外这种抽象也利于与第三方库进行结合，如 Tensorflow、XgBoost 等等。</p>
<p data-nodeid="1012">如何理解本课时内容中的这句话：最后给大家留一个思考题，</p>
<p data-nodeid="1013">训练完成好的模型也是一个 Transformer。</p></div>

</body></html>