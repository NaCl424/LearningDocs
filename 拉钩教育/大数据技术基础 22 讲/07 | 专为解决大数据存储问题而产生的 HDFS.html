<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>07 | 专为解决大数据存储问题而产生的 HDFS</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
  <div id="content"><h1>07 | 专为解决大数据存储问题而产生的 HDFS</h1><p data-nodeid="1697" class="">上一讲中，我们了解了在互联网公司中，数据一般都是从哪里来，以及如何进行数据的采集。而数据采集完成后需要进行长期的保存，以备我们在日后的生产活动中进行各种分析和使用，这就涉及了文件系统的问题，所以在这一讲中，我们就来讲解一下在 Hadoop 体系中的文件系统 HDFS 是如何运转的，同时，我们要动动手，搭建一个简单的 Hadoop 系统，并使用简单的命令感受一下 HDFS 可以进行什么样的操作。</p>
<h3 data-nodeid="1698">文件系统</h3>
<p data-nodeid="1699">首先我们来看一下什么是文件系统。不管是操作系统还是运行的各种软件，抑或是我们所正在介绍的 Hadoop 工具，其实都是一些程序。在运行的时候，这些程序实际上都在存储和搜索数据。正如我们所熟知的，在电脑中，常用的存储有<strong data-nodeid="1842">内存</strong>和<strong data-nodeid="1843">硬盘</strong>两种形式：</p>
<ul data-nodeid="1700">
<li data-nodeid="1701">
<p data-nodeid="1702">内存的速度快，但是价格更贵，所以使用的存储容量较小；</p>
</li>
<li data-nodeid="1703">
<p data-nodeid="1704">硬盘价格便宜，速度要慢很多。</p>
</li>
</ul>
<p data-nodeid="1705">在我们的电脑中一般都使用<strong data-nodeid="1855">硬盘来进行长期存储</strong>，而<strong data-nodeid="1856">内存用来存储程序运行时的数据</strong>。所以，对于我们的硬盘来说，最基本的功能就是读取和写入功能。但是，这里有很多问题需要解决：</p>
<ul data-nodeid="1706">
<li data-nodeid="1707">
<p data-nodeid="1708">硬盘上的位置如何划分；</p>
</li>
<li data-nodeid="1709">
<p data-nodeid="1710">怎么能够尽快在硬盘上找到需要的数据；</p>
</li>
<li data-nodeid="1711">
<p data-nodeid="1712">对于一块空间如何保障读数据和写数据不是同时进行的；</p>
</li>
<li data-nodeid="1713">
<p data-nodeid="1714">……</p>
</li>
</ul>
<p data-nodeid="1715">针对这么多的问题，提出了一个抽象的概念——<strong data-nodeid="1874">文件</strong>。一个文件就是一个单元，占用一个独立的地址空间，<strong data-nodeid="1875">程序可以读取文件或者创建新的文件</strong>。而对这些文件进行管理，解决这些文件的结构、访问、保护、寻址等功能的系统，我们就称为文件系统。而我们所要介绍的 HDFS，全称 Hadoop Distribute File System，也就是<strong data-nodeid="1876">分布式文件系统</strong>的意思，HDFS 是文件系统的一种实例。</p>
<h3 data-nodeid="1716">HDFS 基础</h3>
<p data-nodeid="1717">前面我们已经简单介绍过 HDFS，它可以利用廉价的普通服务器作为其存储，组建一个大规模存储集群，为各类计算提供数据访问的基础。那么 HDFS 的文件系统比起一般的文件系统有什么特色呢？其实 HDFS 文件系统最大的特色就是它在<strong data-nodeid="1883">分布式架构上的处理</strong>，同时 HDFS 的设计适合一次写入，多次读出的场景，且不支持文件的修改，所以不适合反复修改数据的场景。</p>
<h3 data-nodeid="1718">HDFS 的架构</h3>
<p data-nodeid="1719">在了解 HDFS 的整体架构前我们先来理解一下 HDFS 里的一些小知识。</p>
<p data-nodeid="1720">（1）数据块</p>
<p data-nodeid="1721">HDFS 默认最基本的存储单位是 64MB 的数据块（在 2.x 版本中是 128MB），大小通过配置可调。对于存储空间未达到数据块大小的文件，不会占用整个数据块的存储空间。</p>
<p data-nodeid="1722">（2）元数据节点（NameNode）</p>
<p data-nodeid="1723">元数据节点算是 HDFS 中非常重要的一个概念，用于管理文件系统的命令空间，将所有文件和文件夹的元数据保存在文件系统树中，通过在硬盘保存避免丢失，采用文件命名空间镜像（fs image）及修改日志（edit log）方式保存。</p>
<p data-nodeid="1724">（3）数据节点（DataNode）</p>
<p data-nodeid="1725">数据节点即是真正数据存储的地方。</p>
<p data-nodeid="1726">（4）从元数据节点（Secondary NameNode）</p>
<p data-nodeid="1727">从字面来看像是元数据节点的备用节点，但实际不然，它和元数据节点负责不同的事情，主要负责将命名空间镜像与修改日志文件周期性合并，避免文件过大，合并过后文件会同步至元数据节点，同时本地保存一份，以便在出现故障时恢复。</p>
<p data-nodeid="1728">整体架构图如下所示：</p>
<p data-nodeid="1729"><img src="https://s0.lgstatic.com/i/image6/M01/05/40/Cgp9HWAwhySAD6cwAAfhB14J_uE446.png" alt="图片1.png" data-nodeid="1897"></p>
<p data-nodeid="1730">在架构图中，除了我们上述介绍的几种节点，还有一个 Client，即客户端。</p>
<ul data-nodeid="1731">
<li data-nodeid="1732">
<p data-nodeid="1733">客户端是我们平时用来和 HDFS 服务进行交互的部分，客户端中内置了一套文件操作命令来帮助我们访问 HDFS 服务，比如说我们上传文件、下载文件；</p>
</li>
<li data-nodeid="1734">
<p data-nodeid="1735">同时客户端还负责把我们上传的文件按前面说的数据块进行切分，以方便后续的存储；</p>
</li>
<li data-nodeid="1736">
<p data-nodeid="1737">因此，客户端当然也负责与 NameNode 和 DataNode 进行交互以获取文件位置或者读写文件操作等。</p>
</li>
</ul>
<h3 data-nodeid="1738">HDFS 的优缺点</h3>
<h4 data-nodeid="1739">1.优点</h4>
<p data-nodeid="1740">（1）<strong data-nodeid="1908">高容错性。</strong></p>
<p data-nodeid="1741">在 HDFS 文件系统中，数据都会有多个副本。其中的某一个副本丢失（某一个节点的机器损坏）并不影响整体的使用，可以自动恢复。</p>
<p data-nodeid="1742">（2）<strong data-nodeid="1914">适合大数据处理。</strong></p>
<ul data-nodeid="1743">
<li data-nodeid="1744">
<p data-nodeid="1745"><strong data-nodeid="1919">数据规模</strong>：能够处理数据规模达到 GB、TB、甚至 PB 级别的数据；</p>
</li>
<li data-nodeid="1746">
<p data-nodeid="1747"><strong data-nodeid="1924">文件规模</strong>：能够处理百万规模以上的文件数量，相当之大。</p>
</li>
</ul>
<p data-nodeid="1748">（3）<strong data-nodeid="1929">提供数据一致性保障。</strong></p>
<p data-nodeid="1749">（4）任意一个节点所占用的资源较少，可以<strong data-nodeid="1939">在廉价的机器上运行</strong>，<strong data-nodeid="1940">支持线性扩张</strong>。</p>
<h4 data-nodeid="1750">2.缺点</h4>
<p data-nodeid="1751">（1）<strong data-nodeid="1947">不适合低延时数据访问</strong>，比如毫秒级的存储数据，是做不到的。</p>
<p data-nodeid="1752">（2）<strong data-nodeid="1953">无法高效地对大量小文件进行存储</strong>。</p>
<ul data-nodeid="1753">
<li data-nodeid="1754">
<p data-nodeid="1755">存储大量小文件的话，它会占用 NameNode 大量的内存来存储文件、目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的；</p>
</li>
<li data-nodeid="1756">
<p data-nodeid="1757">小文件存储的寻址时间会超过读取时间，它违反了 HDFS 的设计目标。</p>
</li>
</ul>
<p data-nodeid="1758">（3）<strong data-nodeid="1965">不支持并发写入</strong>、<strong data-nodeid="1966">文件随机修改</strong>。</p>
<p data-nodeid="1759">对于一个文件，只能有一个线程写入，不可以多个线程同时写入。基本不能进行文件的修改，只支持<strong data-nodeid="1972">数据的追加</strong>，如果想修改需要使用新文件覆盖整个旧的文件。</p>
<p data-nodeid="1760">了解了 HDFS 的基本构成，下面进入我们的动手环节。因为 HDFS 已经集成在了 Hadoop 系统中，所以我们来尝试安装一个<strong data-nodeid="1978">单节点的 Hadoop 系统</strong>，然后就可以进行 HDFS 操作了。</p>
<h3 data-nodeid="1761">动手安装 Hadoop</h3>
<p data-nodeid="1762">首先介绍一下，我使用的机器操作系统是 Windows 10。因为 Hadoop 需要 Java 的支持，我们先看一下电脑上是否已经安装了 JDK，并且配置好了环境变量。</p>
<p data-nodeid="1763">进入 CMD 命令提示符中，使用下面这个命令查看 Java 版本，如果显示正常，说明已经安装了 Java，并且配置了环境变量。</p>
<pre class="lang-java" data-nodeid="1764"><code data-language="java">C:\Users\userxxx&gt;java -version
java version <span class="hljs-string">"1.8.0_231"</span>
Java(TM) <span class="hljs-function">SE Runtime <span class="hljs-title">Environment</span> <span class="hljs-params">(build <span class="hljs-number">1.8</span><span class="hljs-number">.0</span>_231-b11)</span>
Java <span class="hljs-title">HotSpot</span><span class="hljs-params">(TM)</span> 64-Bit Server <span class="hljs-title">VM</span> <span class="hljs-params">(build <span class="hljs-number">25.231</span>-b11, mixed mode)</span>
</span></code></pre>
<p data-nodeid="1765">在 Windows 8 及以上版本，如果你的 Java&nbsp;JDK 安装在了 Program Files 路径下面，需要注意使用下面的方式来调整你的环境变量路径，否则我们的 Hadoop 配置会无法识别。</p>
<pre class="lang-java" data-nodeid="1766"><code data-language="java">用 “Progra~<span class="hljs-number">1</span>” 替代 “Program Files”
用 “Progra~<span class="hljs-number">2</span>” 替代 “<span class="hljs-function">Program <span class="hljs-title">Files</span><span class="hljs-params">(x86)</span>”
</span></code></pre>
<p data-nodeid="1767">由于在 Windows 系统下支持得并不是很好，原生的 3.2.1 版本可能需要做一些调整，我这里把调整好的项目放到了<a href="https://pan.baidu.com/s/1s2Kk9hQUwYcr_nXyJgzDDQ" data-nodeid="1986">云盘</a>上（提取码：k132），你可以下载我已经打包好的。</p>
<p data-nodeid="1768">下载完，把文件解压到自己的电脑上，比如我这里是放在 D:\，打开 CMD 命令提示符，然后进入 Hadoop 的 bin 路径，如下所示：</p>
<pre class="lang-java" data-nodeid="1769"><code data-language="java">D:\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\bin&gt;
</code></pre>
<p data-nodeid="1770">使用命令 Hadoop Version，如果正常可以看到如下版本信息：</p>
<pre class="lang-java" data-nodeid="1771"><code data-language="java">Hadoop <span class="hljs-number">3.2</span><span class="hljs-number">.1</span>
Source code repository https:<span class="hljs-comment">//gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842</span>
Compiled by rohithsharmaks on <span class="hljs-number">2019</span>-<span class="hljs-number">09</span>-<span class="hljs-number">10</span>T15:<span class="hljs-number">56</span>Z
Compiled with protoc <span class="hljs-number">2.5</span><span class="hljs-number">.0</span>
From source with checksum <span class="hljs-number">776</span>eaf9eee9c0ffc370bcbc1888737
This command was run using /D:/hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>/hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>/share/hadoop/common/hadoop-common-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>.jar
</code></pre>
<p data-nodeid="1772">接下来我们需要修改几个配置文件，让 Hadoop 进行最基本的启动。</p>
<p data-nodeid="1773">（1）修改 D:\hadoop-3.2.1\hadoop-3.2.1\etc\hadoop\core-site.xml 为：</p>
<pre class="lang-java" data-nodeid="1774"><code data-language="java">&lt;configuration&gt;
&lt;property&gt;
&nbsp; &nbsp; &nbsp; &lt;name&gt;fs.defaultFS&lt;/name&gt;
&nbsp; &nbsp; &nbsp; &lt;value&gt;hdfs://localhost:9820&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p data-nodeid="1775">（2）修改 D:\hadoop-3.2.1\hadoop-3.2.1\etc\hadoop\mapred-site.xml 为：</p>
<pre class="lang-java" data-nodeid="1776"><code data-language="java">&lt;configuration&gt;
&nbsp;&nbsp;&nbsp;&lt;property&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;yarn&lt;/value&gt;
&nbsp;&nbsp;&nbsp;&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p data-nodeid="1777">（3）修改 D:\hadoop-3.2.1\hadoop-3.2.1\etc\hadoop\hdfs-site.xml 为：</p>
<pre class="lang-shell" data-nodeid="1778"><code data-language="shell">&lt;configuration&gt;
&lt;property&gt;
&nbsp; &nbsp; &nbsp; &nbsp;&lt;name&gt;dfs.replication&lt;/name&gt;
&nbsp; &nbsp; &nbsp; &nbsp;&lt;value&gt;1&lt;/value&gt;
&nbsp; &nbsp;&lt;/property&gt;
&nbsp; &nbsp;&lt;property&gt;
&nbsp; &nbsp; &nbsp; &nbsp;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&nbsp; &nbsp; &nbsp; &nbsp;&lt;value&gt;file:///d:/hadoop-3.2.1/hadoop-3.2.1/data/dfs/namenode&lt;/value&gt;
&nbsp; &nbsp;&lt;/property&gt;
&nbsp; &nbsp;&lt;property&gt;
&nbsp; &nbsp; &nbsp; &nbsp;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&nbsp; &nbsp; &nbsp;&lt;value&gt;file:///d:/hadoop-3.2.1/hadoop-3.2.1/data/dfs/datanode&lt;/value&gt;
&nbsp; &nbsp;&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p data-nodeid="1779">这里的 value 为 1 表明我们构建的系统只有一个节点，同时，定义了我们的 NameNode 根目录和 DataNode 根目录。</p>
<p data-nodeid="1780">（4）修改 D:\hadoop-3.2.1\hadoop-3.2.1\etc\hadoop\yarn-site.xml 为：</p>
<pre class="lang-java" data-nodeid="1781"><code data-language="java">&lt;configuration&gt;
&nbsp;&nbsp;&nbsp;&lt;property&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;description&gt;Yarn Node Manager Aux Service&lt;/description&gt;
&nbsp;&nbsp;&nbsp;&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p data-nodeid="2121" class="te-preview-highlight">然后输入 hadoop namenode -format，应该能看到这样的结果：</p>

<p data-nodeid="1783"><img src="https://s0.lgstatic.com/i/image6/M00/05/40/Cgp9HWAwh-eAd0mTAASyHJTVzeU268.png" alt="图片2.png" data-nodeid="2041"></p>
<p data-nodeid="1784">选择 Y，可以看到执行成功，如下图所示：</p>
<p data-nodeid="1785"><img src="https://s0.lgstatic.com/i/image6/M01/05/40/Cgp9HWAwh0aAYfOcAATMYgbI4Mc835.png" alt="图片3.png" data-nodeid="2045"></p>
<p data-nodeid="1786">然后在 sbin 目录下输入 start-all，会有多个 CRM 窗口被创建，此时输入 jps，可以看到如下结果：</p>
<p data-nodeid="1787"><img src="https://s0.lgstatic.com/i/image6/M01/05/3D/CioPOWAwh1WAfTRlAAF4sJIqw4E558.png" alt="图片4.png" data-nodeid="2049"></p>
<p data-nodeid="1788">在浏览器中输入<a href="http://localhost:9870/dfshealth.html" data-nodeid="2053">http://localhost:9870/dfshealth.html</a>，你可以看到如下界面：</p>
<p data-nodeid="1789"><img src="https://s0.lgstatic.com/i/image6/M01/05/3D/CioPOWAwh2OAUZq7AAGfdNxsCoE738.png" alt="图片5.png" data-nodeid="2057"></p>
<p data-nodeid="1790">输入<a href="http://localhost:9864/datanode.html" data-nodeid="2061">http://localhost:9864/datanode.html</a>，你可以看到如下监控界面：</p>
<p data-nodeid="1791"><img src="https://s0.lgstatic.com/i/image6/M01/05/40/Cgp9HWAwh3KAYT1OAAGnOAZiR2A626.png" alt="图片6.png" data-nodeid="2065"></p>
<p data-nodeid="1792">输入<a href="http://localhost:8088/cluster" data-nodeid="2069">http://localhost:8088/cluster</a>，你可以看到如下管理界面：</p>
<p data-nodeid="1793"><img src="https://s0.lgstatic.com/i/image6/M00/05/3D/CioPOWAwh4GAFt-IAAK57cMzRLg893.png" alt="图片7.png" data-nodeid="2073"></p>
<p data-nodeid="1794">当年看到这些界面，说明已经部署成功了，我们已经创建了有 1 个节点的 Hadoop 系统。在课后，你可以尝试使用虚拟机来创建具有多个节点的 Hadoop 系统，那么在配置上会有什么不同呢，这个留给你自行探索。</p>
<p data-nodeid="1795">如果你要关闭 Hadoop 服务，记得在 sbin 路径下使用命令：</p>
<pre class="lang-java" data-nodeid="1796"><code data-language="java">.\stop-all
</code></pre>
<p data-nodeid="1797">接下来，我们使用已经部署好的 Hadoop 系统来进行一些 HDFS 的文件操作。</p>
<h3 data-nodeid="1798">HDFS 简单使用</h3>
<p data-nodeid="1799">根据部署的服务，我们的 HDFS 根目录是 <a href="hdfs://localhost:9820/user/" data-nodeid="2081">hdfs://localhost:9820</a>，下面我们尝试在根目录下面创建子目录 user，如下命令所示：</p>
<pre class="lang-java" data-nodeid="1800"><code data-language="java">D:\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\bin&gt;hadoop fs -mkdir&nbsp;&nbsp;hdfs:<span class="hljs-comment">//localhost:9820/user/</span>
</code></pre>
<p data-nodeid="1801">创建目录使用的是 mkdir，接着我们来列出根目录下面的详情，看是否创建成功了。详情如下所示：</p>
<pre class="lang-java" data-nodeid="1802"><code data-language="java">D:\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\bin&gt;hadoop fs -ls&nbsp;&nbsp;hdfs:<span class="hljs-comment">//localhost:9820/</span>
Found <span class="hljs-number">1</span> items
drwxr-xr-x&nbsp;&nbsp;&nbsp;- LonnyHirsi supergroup&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-number">0</span> <span class="hljs-number">2021</span>-<span class="hljs-number">01</span>-<span class="hljs-number">23</span> <span class="hljs-number">17</span>:<span class="hljs-number">49</span> hdfs:<span class="hljs-comment">//localhost:9820/user</span>
</code></pre>
<p data-nodeid="1803">可以看到，这里显示根目录下有一个项，就是我们刚创建的 user 目录。从这两个命令可以了解，HDFS 的文件操作命令与 Linux 的文件操作命令基本相同。</p>
<p data-nodeid="1804">不过要注意，HDFS 中的路径只能一层一层创建，如果我们尝试下面的命令，会得到一个错误信息。</p>
<pre class="lang-java" data-nodeid="1805"><code data-language="java">D:\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\hadoop-<span class="hljs-number">3.2</span><span class="hljs-number">.1</span>\bin&gt;hadoop fs -mkdir&nbsp;&nbsp;hdfs:<span class="hljs-comment">//localhost:9820/data/filedir</span>
mkdir: \`hdfs:<span class="hljs-comment">//localhost:9820/data': No such file or directory</span>
</code></pre>
<p data-nodeid="1806">这是因为我们的根目录下面还没有 data 目录。</p>
<p data-nodeid="1807">除了这些命令，HDFS 的操作还有：</p>
<ul data-nodeid="1808">
<li data-nodeid="1809">
<p data-nodeid="1810">显示文件内容的 cat 命令；</p>
</li>
<li data-nodeid="1811">
<p data-nodeid="1812">上传文件的 put 命令；</p>
</li>
<li data-nodeid="1813">
<p data-nodeid="1814">下载文件的 get 命令；</p>
</li>
<li data-nodeid="1815">
<p data-nodeid="1816">移动文件的 mv 命令；</p>
</li>
<li data-nodeid="1817">
<p data-nodeid="1818">删除文件的 rm 命令；</p>
</li>
<li data-nodeid="1819">
<p data-nodeid="1820">……</p>
</li>
</ul>
<p data-nodeid="1821">如果你需要学习这部分内容可以找一本相关的书籍进行更深入的学习，在这个课程中我们就不再过多地进行介绍了。</p>
<h3 data-nodeid="1822">总结</h3>
<p data-nodeid="1823">这一讲我们讲解了 HDFS 文件系统，它是 Hadoop 体系两大核心基石之一，主要负责存储的部分。另外一部分解决计算问题的 MapReduce 我们会在《11 | MapReduce 处理大数据的基本思想有哪些》详细介绍。</p>
<p data-nodeid="1824">在介绍了 HDFS 的基础架构之后，我们安排了一个实践环节，也就是动手安装 Hadoop 系统，当然我们这里安装的是<strong data-nodeid="2104">单机单节点</strong>，希望你也能够亲自去尝试一下，甚至是用虚拟机搭建一个小型的多机环境以熟悉 Hadoop 的实际情况，在此过程中有任何问题，都欢迎与我进行交流。</p>
<p data-nodeid="1825">课程的最后，简单介绍了 HDFS 的一些使用命令，可以看到跟我们平时在 Linux 系统中使用的文件处理命令基本相同，HDFS 帮我们屏蔽了后端存储的细节，让我们能够顺畅地使用。</p>
<p data-nodeid="1826">下一讲，我们将讲解在 Hadoop 体系中与存储相关的两个项目：数据库 HBase 和数仓工具 Hive，让我们下一讲再见吧。</p>
<hr data-nodeid="1827">
<p data-nodeid="1828"><a href="https://shenceyun.lagou.com/r/rJs" data-nodeid="2111"><img src="https://s0.lgstatic.com/i/image6/M00/00/6D/Cgp9HWAaHaOAI85HAAUCrlmIuEw966.png" alt="Drawing 2.png" data-nodeid="2110"></a></p>
<p data-nodeid="1829"><strong data-nodeid="2115">《大数据开发高薪训练营》</strong></p>
<p data-nodeid="1830" class="">PB 级企业大数据项目实战 + 拉勾硬核内推，5 个月全面掌握大数据核心技能。<a href="https://shenceyun.lagou.com/r/rJs" data-nodeid="2119">点击链接</a>，全面赋能！</p></div>

</body></html>