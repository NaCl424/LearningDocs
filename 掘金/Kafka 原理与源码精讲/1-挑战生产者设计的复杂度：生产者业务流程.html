<!DOCTYPE html><html lang="en"><head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>第 01 讲：设计一份吸引面试官的简历</title>
<style type="text/css">
:root {
    --control-text-color: #777;
    --select-text-bg-color: rgba(223, 197, 223);  /*#7e66992e;*/
    
    /* side bar */
    --side-bar-bg-color: rgb(255, 255, 255);
    --active-file-text-color: #8163bd;
    --active-file-bg-color: #E9E4F0;
    --item-hover-bg-color: #E9E4F0;
    --active-file-border-color: #8163bd;

    --title-color: #6c549c;
    --font-sans-serif: 'Ubuntu', 'Source Sans Pro', sans-serif !important;
    --font-monospace: 'Fira Code', 'Roboto Mono', monospace !important;
    --purple-1: #8163bd;
    --purple-2: #79589F;
    --purple-3: #fd5eb8;
    --purple-light-1: rgba(99, 99, 172, .05);
    --purple-light-2: rgba(99, 99, 172, .1);
    --purple-light-3: rgba(99, 99, 172, .2);
    --purple-light-4: rgba(129, 99, 189, .3);
    --purple-light-5: #E9E4F0;
    --purple-light-6: rgba(129, 99, 189, .8);
}

/* html {
    font-size: 16px;
} */

body {
    font-family: var(--font-sans-serif);
    color: #34495e;
    -webkit-font-smoothing: antialiased;
    line-height: 1.6rem;
    letter-spacing: 0;
    margin: 0;
    overflow-x: hidden;
}

/* 页边距 和 页面大小 */
#write {
    padding-left: 6ch;
    padding-right: 6ch;
    margin: 0 auto;
}

#write p {
    line-height: 1.6rem;
    word-spacing: .05rem;
}

#write ol li {
    padding-left: 0.5rem;
}

#write > ul:first-child,
#write > ol:first-child {
    margin-top: 30px;
}

body > *:first-child {
    margin-top: 0 !important;
}

body > *:last-child {
    margin-bottom: 0 !important;
}

a {
    color: var(--purple-1);
    padding: 0 2px;
    text-decoration: none;
}
.md-content {
    color: var(--purple-light-6);
}
#write a {
    border-bottom: 1px solid var(--purple-1);
    color: var(--purple-1);
    text-decoration: none;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 0.5rem;
    /* font-weight: bold; */
    font-weight: 500 !important;
    line-height: 1.4;
    cursor: text;
    color: var(--title-color);
    font-family: var(--font-sans-serif);
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}

h1 tt,
h1 code {
    font-size: inherit !important;
}
h2 tt,
h2 code {
    font-size: inherit !important;
}
h3 tt,
h3 code {
    font-size: inherit !important;
}
h4 tt,
h4 code {
    font-size: inherit !important;
}
h5 tt,
h5 code {
    font-size: inherit !important;
}
h6 tt,
h6 code {
    font-size: inherit !important;
}


h1 {
    padding-bottom: .4rem;
    font-size: 2.2rem;
    line-height: 1.3;
}
h1 {
    text-align: center;
    padding-bottom: 0.3em;
    font-size: 2.2em;
    line-height: 1.2;
    margin: 2.4em auto 1.2em;
}
h1:after {
    content: '';
    display: block;
    margin: 0.2em auto 0;
    width: 100px;
    height: 2px;
    border-bottom: 2px solid var(--title-color);
}

h2 {
    margin: 1.6em auto 0.5em;
    padding-left: 10px;
    line-height: 1.4;
    font-size: 1.8em;
    border-left: 9px solid var(--title-color);
    border-bottom: 1px solid var(--title-color);
}
h3 {
    font-size: 1.5rem;
    margin: 1.2em auto 0.5em;
}
h4 {
    font-size: 1.3rem;
}
h5 {
    font-size: 1.2rem;
}
h6 {
    font-size: 1.1rem;
}

p,
blockquote,
ul,
ol,
dl,
table {
    margin: 0.8em 0;
}

li > ol,
li > ul {
    margin: 0 0;
}

hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

body > h2:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child {
    margin-top: 0;
    padding-top: 0;
}

body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
    margin-top: 0;
    padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
    margin-top: 0;
    padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
    margin-top: 0;
}

li p.first {
    display: inline-block;
}

ul,
ol {
    padding-left: 30px;
}

ul:first-child,
ol:first-child {
    margin-top: 0;
}

ul:last-child,
ol:last-child {
    margin-bottom: 0;
}

/* 引用 */
blockquote {
    /* margin-left: 1rem; */
    border-left: 4px solid var(--purple-light-4);
    padding: 10px 15px;
    color: #777;
    background-color: var(--purple-light-1);
}

/* 表格 */
table {
    padding: 0;
    word-break: initial;
}

table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}

/* 表格 背景色 */
table tr:nth-child(2n),
thead {
    background-color: var(--purple-light-1);
}
#write table thead th {
    background-color: var(--purple-light-2);
}

table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr td {
    border: 1px solid #dfe2e5;
    text-align: left;
    margin: 0;
    padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

/* 粗体 */
#write strong {
    padding: 0 2px;
    color: var(--purple-1);
}

/* 斜体 */
#write em {
    padding: 0 5px 0 2px;
    /* font-style: normal; */
    color: #42b983;
}

/* inline code */
#write code, tt {
    padding: 2px 4px;
    border-radius: 2px;
    font-family: var(--font-monospace);
    font-size: 0.92rem;
    color: var(--purple-3); 
    background-color: rgba(99, 99, 172, .05);
}

tt {
    margin: 0 2px;
}

#write .md-footnote {
    background-color: #f8f8f8;
    color: var(--purple-3);
}

/* heighlight. */
#write mark {
    background-color: #fbd3ea;
    border-radius: 2px;
    padding: 2px 4px;
    margin: 0 2px;
}

#write del {
    padding: 1px 2px;
}

.md-task-list-item > input {
    margin-left: -1.3em;
}

@media print {
    html {
        font-size: 0.9rem;
    }

    table,
    pre {
        page-break-inside: avoid;
    }

    pre {
        word-wrap: break-word;
    }
}

#write pre.md-meta-block {
    padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
    bottom: .375rem;
}

/* 图片 */
.md-image > .md-meta {
    border-radius: 3px;
    font-family: var(--font-monospace);
    padding: 2px 0 0 4px;
    font-size: 0.9em;
    color: inherit;
}
p .md-image:only-child{
    width: auto;
    text-align: left;
    margin-left: 2rem;
}
.md-tag {
    color: inherit;
}
/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
    /* box-shadow: 0 4px 24px -6px #ddd; */
    box-shadow:var(--purple-light-2) 0px 10px 15px;
}

#write a.md-toc-inner {
    line-height: 1.6;
    white-space: pre-line;
    border-bottom: none;
    font-size: 0.9rem;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

#md-notification:before {
    top: 10px;
}

header,
.context-menu,
.megamenu-content,
footer {
    font-family: var(--font-sans-serif);
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
    visibility: visible;
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

/* 代码框 */
/* CodeMirror 3024 Day theme */

/* 代码段 背景 */
pre {
    --select-text-bg-color: rgba(223, 197, 223) !important;
    margin: .5em 0;
    padding: 1em 1.4em;
    border-radius: 8px;
    background: #f6f8fa;
    overflow-x: auto;
    box-sizing: border-box;
    font-size: 14px;
}

/* 边框 */
.md-fences {
    border: 1px solid #e7eaed;
    border-radius: 3px;
}

.cm-s-inner {
  padding: .25rem;
  border-radius: .25rem;
}

.cm-s-inner.CodeMirror, .cm-s-inner .CodeMirror-gutters {
  background-color: #f8f8f8 !important;
  color: #3a3432 !important;
  border: none;
}

.cm-s-inner .CodeMirror-gutters {
  color: #6d8a88;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: solid thin #5c5855 !important;
}

.cm-s-inner .CodeMirror-linenumber {
  color: #807d7c;
}

.cm-s-inner .CodeMirror-line::selection, .cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: var(--purple-light-2);
}

.cm-s-inner span.cm-comment {
  color: #cdab53;
}

.cm-s-inner span.cm-string, .cm-s-inner span.cm-string-2 {
  color: #f2b01d;
}

.cm-s-inner span.cm-number {
  color: #a34e8f;
}

.cm-s-inner span.cm-variable {
  color: #01a252;
}

.cm-s-inner span.cm-variable-2 {
  color: #01a0e4;
}

.cm-s-inner span.cm-def {
  /* color: #e8bbd0; */
  color: #e2287f;
}

.cm-s-inner span.cm-operator {
  color: #ff79c6;
}

.cm-s-inner span.cm-keyword {
  color: #db2d20;
}

.cm-s-inner span.cm-atom {
  color: #a34e8f;
}

.cm-s-inner span.cm-meta {
  color: inherit;
}

.cm-s-inner span.cm-tag {
  color: #db2d20;
}

.cm-s-inner span.cm-attribute {
  color: #01a252;
}

.cm-s-inner span.cm-qualifier {
  color: #388aa3;
}

.cm-s-inner span.cm-property {
  color: #01a252;
}

.cm-s-inner span.cm-builtin {
  color: #388aa3;
}

.cm-s-inner span.cm-variable-3, .cm-s-inner span.cm-type {
  color: #ffb86c;
}

.cm-s-inner span.cm-bracket {
  color: #3a3432;
}

.cm-s-inner span.cm-link {
  color: #a34e8f;
}

.cm-s-inner span.cm-error {
  background: #db2d20;
  color: #5c5855;
}

/* .md-fences.md-focus .cm-s-inner .CodeMirror-activeline-background {
  background: var(--purple-light-2);
} */

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: #a34e8f !important;
}

#fences-auto-suggest .active {
  background: #ddd;
}

#write .code-tooltip {
  bottom: initial;
  top: calc(100% - 1px);
  background: #f7f7f7;
  border: 1px solid #ddd;
  border-top: 0;
}

.auto-suggest-container {
  border-color: #b4b4b4;
}

.auto-suggest-container .autoComplt-hint.active {
  background: #b4b4b4;
  color: inherit;
}

/* task list */
#write .md-task-list-item > input {
  -webkit-appearance: initial;
  display: block;
  position: absolute;
  border: 1px solid #b4b4b4;
  border-radius: .25rem;
  margin-top: .1rem;
  margin-left: -1.8rem;
  height: 1.2rem;
  width: 1.2rem;
  transition: background 0.3s;
}

#write .md-task-list-item > input:focus {
  outline: none;
  box-shadow: none;
}

#write .md-task-list-item > input:hover {
  background: #ddd;
}

#write .md-task-list-item > input[checked]::before {
  content: '';
  position: absolute;
  top: 20%;
  left: 50%;
  height: 60%;
  width: 2px;
  transform: rotate(40deg);
  background: #333;
}

#write .md-task-list-item > input[checked]::after {
  content: '';
  position: absolute;
  top: 46%;
  left: 25%;
  height: 30%;
  width: 2px;
  transform: rotate(-40deg);
  background: #333;
}

#write .md-task-list-item > p {
  transition: color 0.3s, opacity 0.3s;
}

#write .md-task-list-item.task-list-done > p {
  color: #b4b4b4;
  text-decoration: line-through;
}

#write .md-task-list-item.task-list-done > p > .md-emoji {
  opacity: .5;
}

#write .md-task-list-item.task-list-done > p > .md-link > a {
  opacity: .6;
}

/* sidebar and outline */
.pin-outline .outline-active {
  color: var(--active-file-text-color); 
}

.file-list-item {
    border-bottom: 1px solid;
    border-color: var(--purple-light-5);
}

.file-list-item-summary {
    font-weight: 400;
}

.file-list-item.active {
    color: var(--active-file-text-color);
    background-color: var(--purple-light-5);
}

.file-tree-node.active>.file-node-background {
    background-color: var(--purple-light-5);
    font-weight: 700;
} 

.file-tree-node.active>.file-node-content {
    color: var(--active-file-text-color);
    font-weight: 700;
}

.file-node-content {
    color: #5e676d;
}

.sidebar-tabs {
    border-bottom: none;
}
.sidebar-tab.active {
    font-weight: 400;
}

.sidebar-content-content {
    font-size: 0.9rem;
}

img {
    max-width: 100%;
}

body {
    background-color: rgb(237, 237, 237);
}
#content {
    width: 836px;
    padding: 50px;
    background: #fff;
    margin: 0 auto;
}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/purple.css*/</style><style type="text/css">.hljs{display:block;overflow-x:auto;padding:.5em;color:#383a42;background:#fafafa}.hljs-comment,.hljs-quote{color:#a0a1a7;font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a626a4}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#e45649}.hljs-literal{color:#0184bb}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#50a14f}.hljs-built_in,.hljs-class .hljs-title{color:#c18401}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986801}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#4078f2}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}/*# sourceURL=/Users/young/Documents/Codes/Fun/lagou/public/atom-one-light.min.css*/</style></head>
<body>
<div id="content"><h1>挑战生产者设计的复杂度：生产者业务流程</h1>
<blockquote>
<p>这节课我提供了视频讲解，在源码讲解的基础上增加了原理解析和架构学习的部分，作者本人的B站个人空间如下，可以找到对应的教学视频（教学视频现在已经更新到了生产者，后面的视频还需时间）
<a href="https://www.bilibili.com/video/BV1xY4y1a7LV" target="_blank" rel="nofollow noopener noreferrer">https://www.bilibili.com/video/BV1xY4y1a7LV</a></p>
</blockquote>
<p>这节课我们介绍生产者客户端最上层的<code>主逻辑</code>，底层用到了生产者相关的所有组件。学完这节课后，你会对生产者从宏观层面有很好的掌握，同时对前面的课程也有提纲挈领的作用。</p>
<p>先提前给你介绍这节课要学习的知识点的思维导图：</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b5ea6476f384c559e1fe943209033e4~tplv-k3u1fbpfcp-watermark.image?" alt="1C678534-3404-4CCD-9874-28F375BDCC5E.png"></p>
<h2>生产者主流程</h2>
<p>这里我介绍一下生产者发送消息的主逻辑。下面的图简单描述了消息发送的每一个环节，希望你能结合前面的课程对生产者发送消息的流程有较为深刻的理解。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52977133e84044be996bf8ebab52ff10~tplv-k3u1fbpfcp-watermark.image?" alt="207E1B82-0EF9-4549-B1D5-D072EEC223A0.png"></p>
<p>我描述下每个步骤。</p>
<ol>
<li>ProducerInterceptors：作为<code>拦截器</code>对消息进行拦截，拦截器的主要作用是按照一定的规则统一对消息进行处理。比如要统一给所有消息加全局id，这样消息发送的时候，调用者就不用考虑生成全局id的事情，完全交给拦截器去做。</li>
<li>Serializer：序列化器。对消息的key和value进行<code>序列化</code>。因为在网络传输中数据是以字节码的形式传输的，所以要用序列化器对数据进行序列化。</li>
<li>Partitioner: <code>分区器</code>。消息发送的时候要确定发送到哪个分区，分区器通过算法给消息分配主题分区。</li>
<li>RecordAccumulator:<code>消息积累器</code>。按分区收集消息，实现消息的异步批量发送。</li>
<li>Sender：<code>消息发送器</code>。从 RecordAccumulator 批量获取消息。</li>
<li>构造ClientRequest。ClientRequest 是客户端请求。</li>
<li>将 ClientRequest 交给 NetworkClient，准备发送。</li>
<li>NetworkClient 将请求放入 KafkaChannel 的缓存，等待发送。</li>
<li>KafkaChannel 执行网络IO，发送请求。</li>
<li>NetworkClient收到服务端的响应，调用 ClientRequest 的<code>回调方法</code>。</li>
<li>调用 ProducerBatch 的回调方法，最终会调用到用户在发送消息时注册的回调方法。</li>
</ol>
<h2>类KafkaProducer</h2>
<p>了解完以上主流程和步骤之后，接下来我们分析一下KafkaProducer的源码。</p>
<h3>重要字段</h3>
<pre><code>public class KafkaProducer&#x3C;K, V> implements Producer&#x3C;K, V> {         
  //生产者的id         
  private final String clientId;         
  //分区器         
  private final Partitioner partitioner;         
  //最大的消息长度，默认1M         
  private final int maxRequestSize;         
  //对应BufferPool的缓存区大小         
  private final long totalMemorySize;         
  //生产者元数据         
  private final ProducerMetadata metadata;           
  //消息累加器         
  private final RecordAccumulator accumulator;       
  //执行发送消息的类         
  private final Sender sender;         
  //执行Sender发送消息的线程         
  private final Thread ioThread;         
  //消息压缩的类型         
  private final CompressionType compressionType;   
  //key序列化器         
  private final Serializer&#x3C;K> keySerializer;         
  //value序列化器         
  private final Serializer&#x3C;V> valueSerializer;         
  //生产者客户端参数配置         
  private final ProducerConfig producerConfig;         
  //等待元数据更新的最大时间，默认1分钟         
  private final long maxBlockTimeMs;         
  //拦截器         
  private final ProducerInterceptors&#x3C;K, V> interceptors;        
  private final ApiVersions apiVersions;             
  private final TransactionManager 
</code></pre>
<p>这些重要字段的含义和关键点如下。</p>
<ul>
<li><code>clientId</code>：生产者客户端的id。</li>
<li><code>partitioner</code>：分区器。基于一定的算法把消息分配到某一个分区。</li>
<li><code>maxRequestSize</code>：消息的最大长度，默认1M。</li>
<li><code>totalMemorySize</code>：发送消息的缓冲区大小，默认32M。</li>
<li><code>metadata</code>：集群的元数据。</li>
<li><code>accumulator</code>：RecordAccumulator类对象。负责缓冲消息。</li>
<li><code>sender</code>：Sender类对象。负责发送消息。</li>
<li><code>ioThread</code>：KafkaThread类对象，在这里负责封装Sender类。</li>
<li><code>compressionType</code>：消息压缩的类型。</li>
<li><code>keySerializer</code>：key的序列化器。</li>
<li><code>valueSerializer</code>：value的序列化器。</li>
<li><code>producerConfig</code>：生产者客户端的配置参数。</li>
<li><code>maxBlockTimeMs</code>：等待元数据更新的最长时间，默认1分钟。</li>
<li><code>interceptors</code>：拦截器。负责在消息发送前或响应接收后统一对消息进行拦截和处理。</li>
</ul>
<h3>构造方法</h3>
<p>生产者发送消息会涉及很多的组件，<strong>KafkaProducer类的构造方法就是初始化生产消息需要的组件</strong>。你先看下源码：</p>
<pre><code>KafkaProducer(Map&#x3C;String, Object> configs,
              Serializer&#x3C;K> keySerializer,
              Serializer&#x3C;V> valueSerializer,
              ProducerMetadata metadata,
              KafkaClient kafkaClient,
              ProducerInterceptors&#x3C;K, V> interceptors,
              Time time) {
    ProducerConfig config = new ProducerConfig(ProducerConfig.appendSerializerToConfig(configs, keySerializer,
            valueSerializer));
    try {
        //1.用户自定义参数
        Map&#x3C;String, Object> userProvidedConfigs = config.originals();
        this.producerConfig = config;
        this.time = time;
        String transactionalId = (String) userProvidedConfigs.get(ProducerConfig.TRANSACTIONAL_ID_CONFIG);
        //2.获取配置参数。
        this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);
        LogContext logContext;
        ......忽略
        //3.获取分区器。
        this.partitioner = config.getConfiguredInstance(ProducerConfig.PARTITIONER_CLASS_CONFIG, Partitioner.class);
        //4.失败重试的退避时间。默认100ms
        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);
        //5.定义key和value的序列化器
        if (keySerializer == null) {
            this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                                                                                     Serializer.class);
            this.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), true);
        } else {
            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);
            this.keySerializer = keySerializer;
        }
        if (valueSerializer == null) {
            this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                                                                                       Serializer.class);
            this.valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), false);
        } else {
            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);
            this.valueSerializer = valueSerializer;
        }
        userProvidedConfigs.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);
        ProducerConfig configWithClientId = new ProducerConfig(userProvidedConfigs, false);
        //6.定义拦截器列表
        List&#x3C;ProducerInterceptor&#x3C;K, V>> interceptorList = (List) configWithClientId.getConfiguredInstances(
                ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProducerInterceptor.class);
        if (interceptors != null)
            this.interceptors = interceptors;
        else
            this.interceptors = new ProducerInterceptors&#x3C;>(interceptorList);
        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,
                valueSerializer, interceptorList, reporters);
        //7.最大请求大小。默认1M,这个值有些小，在实际生产环境中经常会比这个参数大，我们一般设置为10M
        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);
        //8.消息缓冲区大小。默认是32M，如果有特殊的需要我们可以修改
        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);
        //9.获取压缩类型
        this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));
        //10.获取请求最长
        this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);
        //11.设置消息投递超时时间
        int deliveryTimeoutMs = configureDeliveryTimeout(config, log);
        this.apiVersions = new ApiVersions();
        this.transactionManager = configureTransactionState(config, logContext);
        //12.定义缓冲区
        this.accumulator = new RecordAccumulator(logContext,
                config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),
                this.compressionType,
                lingerMs(config),
                retryBackoffMs,
                deliveryTimeoutMs,
                metrics,
                PRODUCER_METRIC_GROUP_NAME,
                time,
                apiVersions,
                transactionManager,
                new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));
        List&#x3C;InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
                config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),
                config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));
        //13，metadata包含了kafka集群元素信息，主要有：kafka集群的节点有哪些，有哪些topic
//每个topic有哪些分区，topic的ISR列表，ISR列表分布在哪些节点上，leader partition在哪些节点上。
//要想获得metadata需要向集群请求获得。         
        if (metadata != null) {
            this.metadata = metadata;
        } else {
            this.metadata = new ProducerMetadata(retryBackoffMs,
                    config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
                    config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),
                    logContext,
                    clusterResourceListeners,
                    Time.SYSTEM);
            this.metadata.bootstrap(addresses);
        }
        this.errors = this.metrics.sensor("errors");
        //14.初始化sender线程类
        this.sender = newSender(logContext, kafkaClient, this.metadata);
        String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
        this.ioThread.start();
        config.logUnused();
        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());
        log.debug("Kafka producer started");
    } catch (Throwable t) {
        close(Duration.ofMillis(0), true);
        throw new KafkaException("Failed to construct kafka producer", t);
    }
}
</code></pre>
<p>第一步，得到用户自定义的参数，用户自定义的参数是从客户端加载配置文件读出来的。</p>
<p>第二步，获取配置的客户端生产者的id。</p>
<p>第三步，获取分区器。分区器是可以用户自己定义的，如果没有自定义的分区器就会用自定义的分区器。分区器是用来给消息分配要发送主题的分区的。</p>
<p>第四步，获取失败重试的退避时间。在客户端请求服务端时，可能因为网络或服务端不能正常对外提供服务造成请求超时。一般情况下，请求失败一次会重试，但是如果重试的频率过高有可能造成服务端网络拥堵。所以，重试必须要等一段时间再请求，这就是重试退避时间的由来，Kafka默认是100ms。</p>
<p>接下来，会初始化key的序列化器和value的序列化器。key和value的序列化器是用户在定义KafkaProducer的时候自定义的。</p>
<p>第七步，是消息大小的最大值。默认是1M，如果超过了最大值会报异常。在生产环境中1M一般是不够用的，建议大家配置10M的大小。</p>
<p>第八步，设置缓冲区的大小，默认是32M。</p>
<p>第九步，设置消息压缩的类型。默认是none，表示默认不压缩。在消息发送的过程中，为了提升发送消息的吞吐量会把消息进行压缩再发送。可以定义我们认为合适的压缩类型。</p>
<p>第十步，设置最大的阻塞时间，默认60S。这里的最大阻塞时间是指从消息开始发送到把消息发送到缓存区消耗的最大时间。</p>
<p>第十一步，设置消息投递超时时间，默认120S。消息投递时间是从发送到收到响应的时间。</p>
<p>第十二步，初始化accumulator，设置了几个参数，我选几个重要的讲下。</p>
<ul>
<li><code>batchSize</code>：批次大小，默认16k。</li>
<li><code>lingerMs</code>：消息批次延迟多久再发送。因为如果消息生产比较慢，而发送比较频繁会造成许多批次没有装满消息就发送出去了，设置延迟时间后批次会尽量积累较多的消息再发送出去。优点是减少了网络请求的次数，缺点是消息发送会人为地延迟。</li>
</ul>
<p>第十三步，初始化元数据对象，metadata是保存在客户端内存中，并与服务端真实的元数据保持准实时数据一致。metadata包含了Kafka集群元素信息，主要有：Kafka集群的节点有哪些，有哪些topic，每个topic有哪些分区，topic的ISR列表，ISR列表分布在哪些节点上，leader partition在哪些节点上，等等。</p>
<p>metadata的数据是要通过网络请求服务端获得的，这里做的仅仅是初始化，而初始化MetaData的时候会设置几个重要参数。</p>
<ul>
<li><code>retryBackoffMs</code>：重试退避时间。默认100MS。metadata的数据是请求服务端获得的，这就有可能失败，失败就会涉及到重试，重试太频繁会导致网络拥堵等问题，所以要设置合适的重试间隔时间。</li>
<li><code>metadata.max.age.ms</code>：元数据过期时间。默认5分钟。由于分区变动或机器中有的机器宕机等原因造成元数据的变动，为了保持一致，客户端需要定时请求服务端更新元数据，这里设置的参数就是元数据过期时间。</li>
</ul>
<p>第十四步，初始化sender，同时也初始化了sender的底层组件NetworkClient，NetworkClient为Sender提供了网络IO的功能。</p>
<p>第十五步，封装并启动sender线程。用ioThread线程类封装Sender线程类，并用ioThread从后台启动Sender线程类。</p>
<p>好，KafkaProducer的字段和构造方法介绍完了，下面介绍<code>异步发送消息</code>的方法。</p>
<h3>方法doSend()</h3>
<p><strong>这个方法的功能是把消息发送到缓冲区，然后直接返回而不会真的发送消息，而真正的发送是等待Sender线程把消息从缓冲区取出来再发送</strong>。相当于异步发送消息。这样的异步发送消息的设计性能和效率都很高。好，下面是发送消息的源码，我来给大家讲解一下。
 </p>
<pre><code>private Future&#x3C;RecordMetadata> doSend(ProducerRecord&#x3C;K, V> record, Callback callback) {
    TopicPartition tp = null;
    try {
        throwIfProducerClosed();
        long nowMs = time.milliseconds();
        ClusterAndWaitTime clusterAndWaitTime;
        try {
            //1.等待元数据更新
            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);
        } catch (KafkaException e) {
            if (metadata.isClosed())
                throw new KafkaException("Producer closed while send in progress", e);
            throw e;
        }
        nowMs += clusterAndWaitTime.waitedOnMetadataMs;
        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
        Cluster cluster = clusterAndWaitTime.cluster;
        byte[] serializedKey;
        //2.序列化key
        try {
            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
                    " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
                    " specified in key.serializer", cce);
        }
        byte[] serializedValue;
        //3.序列化 value
        try {
            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
        } catch (ClassCastException cce) {
            throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
                    " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
                    " specified in value.serializer", cce);
        }
        //4.消息路由到分区。
        int partition = partition(record, serializedKey, serializedValue, cluster);
        tp = new TopicPartition(record.topic(), partition);


        setReadOnly(record.headers());
        Header[] headers = record.headers().toArray();


        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),
                compressionType, serializedKey, serializedValue, headers);
        //5.验证消息的大小
        ensureValidRecordSize(serializedSize);
        long timestamp = record.timestamp() == null ? nowMs : record.timestamp();
        if (log.isTraceEnabled()) {
            log.trace("Attempting to append record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
        }
        // 6.把回调方法和拦截器组装成一个对象
        Callback interceptCallback = new InterceptorCallback&#x3C;>(callback, this.interceptors, tp);


        if (transactionManager != null &#x26;&#x26; transactionManager.isTransactional()) {
            transactionManager.failIfNotReadyForSend();
        }
        // 7.把消息加到缓冲区中
        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                serializedValue, headers, interceptCallback, remainingWaitMs, true, nowMs);


        if (result.abortForNewBatch) {
            int prevPartition = partition;
            partitioner.onNewBatch(record.topic(), cluster, prevPartition);
            partition = partition(record, serializedKey, serializedValue, cluster);
            tp = new TopicPartition(record.topic(), partition);
            if (log.isTraceEnabled()) {
                log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);
            }
            interceptCallback = new InterceptorCallback&#x3C;>(callback, this.interceptors, tp);


            result = accumulator.append(tp, timestamp, serializedKey,
                serializedValue, headers, interceptCallback, remainingWaitMs, false, nowMs);
        }


        if (transactionManager != null &#x26;&#x26; transactionManager.isTransactional())
            transactionManager.maybeAddPartitionToTransaction(tp);
        // 8.唤醒sender线程。
        if (result.batchIsFull || result.newBatchCreated) {
            log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
            this.sender.wakeup();
        }
        return result.future;
    } catch (ApiException e) {
        log.debug("Exception occurred during message send:", e);
        if (callback != null)
            callback.onCompletion(null, e);
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        return new FutureFailure(e);
    } catch (InterruptedException e) {
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        throw new InterruptException(e);
    } catch (KafkaException e) {
        this.errors.record();
        this.interceptors.onSendError(record, tp, e);
        throw e;
    } catch (Exception e) {
        this.interceptors.onSendError(record, tp, e);
        throw e;
    }
}
</code></pre>
<p>我们来分析下发送消息的逻辑。</p>
<p>第一步，调用waitOnMetadata()方法同步等待拉取元数据，如果生产端内存中没有主题相关的分区就会发送网络请求服务端更新最新的元数据信息。同时会把最大阻塞时间参数maxBlockTimeMs传进去，在方法waitOnMetadata()返回后根据方法耗费的时间计算阻塞时间还剩多少，剩余等待时间等于最大阻塞时间减去等待元数据更新的时间。</p>
<p>第二步，根据提供的序列化类型序列化key。</p>
<p>第三步，根据提供的序列化类型序列化value。</p>
<p>第四步，获取消息要路由到的分区。根据消息，序列化后的key、value、元数据分配对应的分区。</p>
<p>第五步，计算序列化后的消息大小，判断是否超出对消息大小的限制。具体判断逻辑如下：</p>
<pre><code>private void ensureValidRecordSize(int size) {
    //消息大小的限制，默认1M
    if (size > maxRequestSize)
        throw new RecordTooLargeException("The message is " + size +
                " bytes when serialized which is larger than " + maxRequestSize + ", which is the value of the " +
                ProducerConfig.MAX_REQUEST_SIZE_CONFIG + " configuration.");
                //缓冲区大小限制，默认32M
    if (size > totalMemorySize)
        throw new RecordTooLargeException("The message is " + size +
                " bytes when serialized which is larger than the total memory buffer you have configured with the " +
                ProducerConfig.BUFFER_MEMORY_CONFIG +
                " configuration.");
}
</code></pre>
<p>主要有两个条件的限制。</p>
<ul>
<li>maxRequestSize：默认1M，可以根据实际情况配置。大于这个数就会抛出相应的异常。</li>
<li>totalMemorySize：默认32M，可以根据实际情况配置。大于这个数就会抛出相应的异常。</li>
</ul>
<p>第六步，把回调方法和拦截器组装成一个对象。后面的步骤会统一给消息配置上这个对象。回调方法是为了收到响应时返回生产者响应的数据，拦截器对响应的数据有过滤功能。</p>
<p>第七步，把消息加到缓冲区的批次里。</p>
<p>第八步，唤醒sender线程。加入到缓存区以后，根据返回判断缓存区对应的批次是否满了或新创建了数据，如果满足条件说明满足发送条件了，这时候要唤醒sender线程，sender线程会从缓冲区抽取批次进程发送。</p>
<h2>获取分区</h2>
<p>接下来我们根据源码讨论下分区，doSend()方法的第四步是选择消息要发送的分区，我们从这里开始分析，如下面的源码所示：</p>
<pre><code>private int partition(ProducerRecord&#x3C;K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
    Integer partition = record.partition();
    return partition != null ?
            partition :
            partitioner.partition(
                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
}
</code></pre>
<p>先判断消息里是否指定了要发送的partition，如果指定了，就用指定的partition；如没有指定，则用分区器选择分区。当我们没有指定分区器的时候，会用默认的分区器DefaultPartitioner。</p>
<h3>类DefaultPartitioner</h3>
<p>接下来我们分析下DefaultPartitioner的源码中的分区方法partition()。</p>
<pre><code>public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster,
                     int numPartitions) {
    //如果没有key就用stickyPartition的策略选取分区
    if (keyBytes == null) {
        return stickyPartitionCache.partition(topic, cluster);
    }
    // hash the keyBytes to choose a partition
    // 如果有key，就用key对分区数哈希取模获取分区
    return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
}
</code></pre>
<p>如果有key，就用key对分区数哈希取模获取分区，如果没有key就用stickyPartition的策略选取分区。</p>
<p>我们接下来分析stickyPartition的策略，stickyPartition的策略是在StickyPartitionCache类里实现的。</p>
<h3>类StickyPartitionCache</h3>
<p>这个类的核心逻辑就是当存在无key的序列消息时，我们消息发送的分区优先保持粘连，如果当前分区下的batch已经满了或者 linger.ms延迟时间已到开始发送，就会重新启动一个新的分区。流程图如下：</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/42ac7e97dd10456c8118d46c836be885~tplv-k3u1fbpfcp-watermark.image?" alt="C17294C9-D987-43A4-B6E9-8BB3449EC5D0.png"></p>
<p>这样设计的好处是：可以最大限度地保障每个batch的消息足够多，并且不至于会有过多的空batch提前申请，因为轮询分区模式下，一组序列消息总是会被分散到各个分区中，会导致每个batch的消息不够大，最终会导致客户端请求频次过多，而Sticky的模式可以降低请求频次。</p>
<p>源码在下面，我给大家讲解一下：</p>
<pre><code>public class StickyPartitionCache {
    //保存给主题分配的对应分区的集合。
    private final ConcurrentMap&#x3C;String, Integer> indexCache;
    public StickyPartitionCache() {
        this.indexCache = new ConcurrentHashMap&#x3C;>();
    }
    //返回主题对应的分区
    public int partition(String topic, Cluster cluster) {
        Integer part = indexCache.get(topic);
        if (part == null) {
            return nextPartition(topic, cluster, -1);
        }
        return part;
    }
    //换取topic对应发送分区
    public int nextPartition(String topic, Cluster cluster, int prevPartition) {
        List&#x3C;PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        Integer oldPart = indexCache.get(topic);
        Integer newPart = oldPart;
        // 分区没有设置对应的分区或因为新的批次触发了需要换分区的需求
        if (oldPart == null || oldPart == prevPartition) {
            List&#x3C;PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
            //没有可用的分区
            if (availablePartitions.size() &#x3C; 1) {
                Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
                newPart = random % partitions.size();
                //只有一个可用的分区
            } else if (availablePartitions.size() == 1) {
                newPart = availablePartitions.get(0).partition();
            } else {
                // 1.newPart == null：给新主题申请发送分区
                // 2.newPart.equals(oldPart)：对于原来就分配了分区的会循环哈希到跟上个不一样的分区
                while (newPart == null || newPart.equals(oldPart)) {
                    Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
                    newPart = availablePartitions.get(random % availablePartitions.size()).partition();
                }
            }
            if (oldPart == null) {
                indexCache.putIfAbsent(topic, newPart);
            } else {
                //有可能被别的线程提前申请了新的分区
                indexCache.replace(topic, prevPartition, newPart);
            }
            return indexCache.get(topic);
        }
        //返回要发送的分区
        return indexCache.get(topic);
    }
}
</code></pre>
<ul>
<li><strong>字段indexCache</strong>。这是一个map集合，key是主题，value是要发送到这个主题的分区编号。</li>
<li><strong>方法partition()</strong>。如果indexCache中有主题对应的分区，就返回这个分区；如果没有，就给新的主题申请分区.</li>
<li><strong>方法nextPartiion()</strong>。调用这个方法的时机有两个：一个是主题对应的发送分区为空，需要新分配分区；另一个是KafkaProducer写缓冲区时一个批次满了需要换批次，这时也需要换主题对应的分区。</li>
</ul>
<h2>总结</h2>
<p>在这节课中，我首先给你讲解了主线程和Sender线程的工作流程，以及涉及到的模块。然后根据源码介绍了KafkaProducer是如何把消息发送到缓存区的，包括获取元数据、序列化key和value、选择分区、验证消息大小是否合法、唤醒Sender线程。最后，还分析了Kafka默认分区的逻辑，以及sticky partition给网络性能带来的好处。</p></div>
</body></html>